{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/drpetros11111/deep-learning-with-python-notebooks/blob/pp_study/02_DataRepr_tf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSAmxRwICQ2K"
      },
      "source": [
        "# Data representations for neural networks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "rQufhR1sDFaV"
      },
      "outputs": [],
      "source": [
        "# TensorFlow for Linear Algebra and Matrix Operations\n",
        "import tensorflow as tf\n",
        "from tensorflow.linalg import inv, det, eig, svd, norm, cholesky, solve, eye, diag\n",
        "from tensorflow import matmul, reshape, transpose, reduce_sum\n",
        "\n",
        "# Matplotlib for plotting\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Optional: NumPy for array handling (if needed for some use cases)\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4r4D6_ACXy8"
      },
      "source": [
        "## Scalars (rank-0 tensors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5K-oTnU7CB5x",
        "outputId": "1bf9933c-4bd9-4dfc-d218-eb1381b0d138"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(12)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import numpy as np\n",
        "x = np.array(12)\n",
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5uxc7JVoDjeX",
        "outputId": "d5075652-b6eb-4506-ed68-4029f0886f2a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "x.ndim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqyleAgSD5AD"
      },
      "source": [
        "## Vectors (rank-1 tensors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "sZyjHAh8DyLt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5584345-08a8-47ab-b4a0-1334831cd530"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([12,  3,  6, 14,  7])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "x = np.array([12, 3, 6, 14, 7])\n",
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0McyaQdJEGxo",
        "outputId": "fa5f6a1f-04cd-429b-c542-7150f60ec001"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "x =np.array([12, 3, 6, 14, 7 ])\n",
        "x\n",
        "x.ndim\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GMdwON6FZED"
      },
      "source": [
        "# Syntax of a function call\n",
        "In the expression x = np.array([12, 3, 6, 14, 7]), both parentheses and brackets serve specific purposes:\n",
        "\n",
        "------------------------------------\n",
        "##Breakdown of the Syntax:\n",
        "    np.array(...)\n",
        "\n",
        "##Parentheses (...):\n",
        "\n",
        "The function np.array() is called with its argument(s) inside parentheses.\n",
        "\n",
        "In Python, parentheses are used to call functions and pass arguments to them.\n",
        "\n",
        "Here, you are calling the array function from the numpy library (aliased as np), which converts a list (or other array-like structures) into a NumPy array.\n",
        "\n",
        "     [12, 3, 6, 14, 7]:\n",
        "\n",
        "##Brackets [...]:\n",
        "\n",
        "The square brackets are used to create a list in Python. In this case, [12, 3, 6, 14, 7] is a list of integers.\n",
        "\n",
        "This list is passed as an argument to the np.array() function, which converts it into a NumPy array.\n",
        "\n",
        "------------------------------\n",
        "#Summary\n",
        "Function Call: The parentheses are necessary for calling the np.array function.\n",
        "\n",
        "List Creation: The brackets are used to define the list of values that you want to convert into an array."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ew77wA9rGs09"
      },
      "source": [
        "# Matrices (rank-2 tensors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S4SetywsGx4t",
        "outputId": "5c428d90-5448-46b0-f112-0e6e91e02f36"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "x = np.array([[5, 78, 2, 34, 0],\n",
        "              [6, 79, 3, 35, 1],\n",
        "              [7, 80, 4, 36, 2]])\n",
        "x.ndim\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eH3hR0GVHWX8",
        "outputId": "cd20435d-14e6-447c-ab9e-e6698781e245"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3, 5)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UgfmG-buHfUU",
        "outputId": "f07f5775-0810-4608-86ac-d49cd29bf677"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "y = np.array([[1, 2, 3, 4, 5],\n",
        "              [6, 7, 8, 9, 99],\n",
        "              [7, 77, 9, 88, 11]])\n",
        "y.ndim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KfjTqoZWIJTp",
        "outputId": "55a6b5da-3e21-4a37-a6db-2c113163e91e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3, 5)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "y.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUemhwCFINc4"
      },
      "source": [
        "# Rank-3 and higher-rank tensors"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The gears of neural networks: tensor operations"
      ],
      "metadata": {
        "id": "9d9fbNqlbaA9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Element-wise operations"
      ],
      "metadata": {
        "id": "Qi1dOg5CbhFJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "g3ksQvAWIV0b"
      },
      "outputs": [],
      "source": [
        "def naive_relu(x):\n",
        "    assert len(x.shape) == 2\n",
        "    x = x.copy()\n",
        "    for i in range(x.shape[0]):\n",
        "        for j in range(x.shape[1]):\n",
        "            x[i, j] = max(x[i, j], 0)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Naive Relu\n",
        "This function, naive_relu, is an implementation of the ReLU (Rectified Linear Unit) activation function, applied manually (or \"naively\") to a 2D array (matrix) x.\n",
        "\n",
        "-------------------------------------\n",
        "--------------------------------------\n",
        "# Explanation of the Function\n",
        "\n",
        "##Input Validation:\n",
        "    assert len(x.shape) == 2\n",
        "\n",
        "\n",
        "This assert statement checks that the input x is a 2D array (has exactly 2 dimensions).\n",
        "\n",
        "If x doesn't have 2 dimensions, the function will raise an assertion error.\n",
        "\n",
        "----------------------------------------\n",
        "\n",
        "##Copying the Input:\n",
        "    x = x.copy()\n",
        "\n",
        "A copy of the input x is made to avoid modifying the original array (this is important for preserving the input when working with arrays in-place).\n",
        "\n",
        "-------------------------------------\n",
        "##Iterating Over the Array:\n",
        "\n",
        "    for i in range(x.shape[0]):\n",
        "       for j in range(x.shape[1]):\n",
        "           x[i, j] = max(x[i, j], 0)\n",
        "\n",
        "The function uses two nested loops to iterate over every element in the 2D array x.\n",
        "\n",
        "x.shape[0] refers to the number of rows.\n",
        "\n",
        "x.shape[1] refers to the number of columns.\n",
        "\n",
        "For each element x[i, j], the ReLU operation is applied:\n",
        "x[i, j] = max(x[i, j], 0) replaces each element with the maximum of the value itself or 0.\n",
        "\n",
        "In other words, if the element is negative, it becomes 0 (ReLU operation).\n",
        "\n",
        "------------------------\n",
        "#Return the Result\n",
        "\n",
        "    return x\n",
        "\n",
        "The modified array x is returned as output after applying the ReLU operation to all its elements.\n",
        "\n",
        "------------------------\n",
        "#What is ReLU?\n",
        "ReLU is a common activation function used in neural networks. It stands for Rectified Linear Unit.\n",
        "Mathematically, ReLU is defined as:\n",
        "\n",
        "$$ReLU(x)=max(0,x)$$\n",
        "\n",
        "If the input is positive, it returns the input.\n",
        "\n",
        "If the input is negative, it returns 0.\n",
        "\n",
        "--------------------------------\n",
        "#Example\n",
        "If x is a 2x2 matrix:\n",
        "\n",
        "    x = np.array([[1, -2], [-3, 4]])\n",
        "\n",
        "The naive_relu(x) function will:\n",
        "\n",
        "Loop through each element.\n",
        "\n",
        "Apply the ReLU operation (replace negative values with 0).\n",
        "\n",
        "-----------------------\n",
        "#Return:\n",
        "    array([[1, 0],\n",
        "           [0, 4]])\n",
        "\n",
        "----------------------------\n",
        "#Summary\n",
        "The naive_relu function manually implements the ReLU activation function for a 2D array using a nested loop.\n",
        "\n",
        "It sets any negative values in the input matrix to 0 while keeping positive values unchanged.\n",
        "\n",
        "This is a basic, non-optimized implementation, and libraries like TensorFlow or NumPy provide much more efficient ways to apply ReLU on large arrays.\n",
        "\n",
        "Would you like to see a more optimized version?"
      ],
      "metadata": {
        "id": "yhdZUD7Ucb2p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def naive_add(x, y):\n",
        "    assert len(x.shape) == 2\n",
        "    assert x.shape == y.shape\n",
        "    x = x.copy()\n",
        "    for i in range(x.shape[0]):\n",
        "        for j in range(x.shape[1]):\n",
        "            x[i, j] += y[i, j]\n",
        "    return x"
      ],
      "metadata": {
        "id": "G0iQUKp3iumf"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "x = np.random.random((20, 100))\n",
        "y = np.random.random((20, 100))\n",
        "\n",
        "t0 = time.time()\n",
        "for _ in range(1000):\n",
        "    z = x + y\n",
        "    z = np.maximum(z, 0.)\n",
        "print(\"Took: {0:.2f} s\".format(time.time() - t0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8OACluf9izwK",
        "outputId": "55491e40-2fbc-4fe5-d96f-7996598eef2e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Took: 0.01 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "x = np.random.random((20, 100))\n",
        "y = np.random.random((20, 100))\n",
        "\n",
        "t0 = time.time()\n",
        "for _ in range(1000):\n",
        "    z = x + y\n",
        "    z = np.maximum(z, 0.)\n",
        "print(\"Took: {0:.2f} s\".format(time.time() - t0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fN_uVFRRjXXe",
        "outputId": "a76fa7af-a54c-4a82-e5e5-e7e65a1dd43f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Took: 0.01 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Benchmark Relu\n",
        "This code snippet benchmarks how long it takes to perform a vectorized operation on two 2D NumPy arrays (matrices) 1,000 times.\n",
        "\n",
        "-----------------------------\n",
        "##Importing the time Module:\n",
        "\n",
        "    import time\n",
        "\n",
        "The time module is used to measure the time it takes to perform certain operations (in this case, to benchmark the speed of adding two arrays and applying the ReLU-like operation).\n",
        "\n",
        "-----------------------------------\n",
        "## Creating Random Arrays:\n",
        "\n",
        "    x = np.random.random((20, 100))\n",
        "    y = np.random.random((20, 100))\n",
        "\n",
        "Two random arrays, x and y, are created using NumPy's random.random function.\n",
        "\n",
        "Both arrays have the shape (20, 100), meaning they are 2D arrays with 20 rows and 100 columns.\n",
        "\n",
        "Each element in x and y is a random float between 0 and 1.\n",
        "\n",
        "-----------------------------\n",
        "##Timing the Operation:\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "t0 stores the current time at the start of the benchmark using time.time().\n",
        "\n",
        "-----------------------------------\n",
        "##Performing the Operation 1,000 Times:\n",
        "\n",
        "    for _ in range(1000):\n",
        "       z = x + y\n",
        "       z = np.maximum(z, 0.)\n",
        "\n",
        "A loop runs 1,000 times. Inside the loop:\n",
        "Addition: z = x + y adds the two arrays x and y element-wise.\n",
        "\n",
        "The result, z, is the same shape as x and y.\n",
        "\n",
        "--------------------------------\n",
        "##ReLU Operation:\n",
        "z = np.maximum(z, 0.) replaces all negative values in z with 0, which is similar to applying the ReLU activation function.\n",
        "\n",
        "Since x and y are filled with random values between 0 and 1, the addition itself won't result in negative values, but this simulates how ReLU is typically applied.\n",
        "\n",
        "------------------------------------\n",
        "##Measuring the Total Time:\n",
        "\n",
        "    print(\"Took: {0:.2f} s\".format(time.time() - t0))\n",
        "\n",
        "After the loop finishes, the total time taken is calculated by subtracting the initial time t0 from the current time using time.time().\n",
        "\n",
        "The result is printed in seconds, formatted to 2 decimal places.\n",
        "\n",
        "This code demonstrates the efficiency of vectorized operations in NumPy:\n",
        "\n",
        "Addition of two arrays (x + y) and applying the ReLU-like operation (np.maximum(z, 0.)) are done in a vectorized manner, meaning the operations are performed on entire arrays at once without the need for explicit loops in Python.\n",
        "\n",
        "NumPy's internal C and Fortran optimizations make such vectorized operations much faster than if we were to use explicit Python loops.\n",
        "\n",
        "--------------------------------\n",
        "##Example Output:\n",
        "An example output might look like:\n",
        "\n",
        "    Took: 0.05 s\n",
        "\n",
        "This indicates that the operation took 0.05 seconds to complete 1,000 iterations.\n",
        "\n",
        "--------------------------------\n",
        "#Summary\n",
        "The code adds two random arrays (x and y) 1,000 times and applies a ReLU-like function to the result.\n",
        "\n",
        "It uses vectorized NumPy operations for efficient computation.\n",
        "The time taken to complete the operation is measured and printed.\n",
        "\n",
        "Would you like to optimize this further or add other operations to measure?"
      ],
      "metadata": {
        "id": "bW7E1ZZcjbBg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Broadcasting"
      ],
      "metadata": {
        "id": "nr0JquS1rEBR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "X = np.random.random((32, 10))\n",
        "y = np.random.random((10,))"
      ],
      "metadata": {
        "id": "ZJlj4vFfoqhF"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = np.expand_dims(y, axis=0)"
      ],
      "metadata": {
        "id": "zeNax11CrScJ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y = np.concatenate([y] * 32, axis=0)"
      ],
      "metadata": {
        "id": "QRVMW80jrYOG"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def naive_add_matrix_and_vector(x, y):\n",
        "    assert len(x.shape) == 2\n",
        "    assert len(y.shape) == 1\n",
        "    assert x.shape[1] == y.shape[0]\n",
        "    x = x.copy()\n",
        "    for i in range(x.shape[0]):\n",
        "        for j in range(x.shape[1]):\n",
        "            x[i, j] += y[j]\n",
        "    return x"
      ],
      "metadata": {
        "id": "MTQQb5K2rePt"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adding a vector to each row of a matrix\n",
        "The function naive_add_matrix_and_vector is a manual implementation of adding a vector to each row of a matrix.\n",
        "\n",
        "This is known as broadcasting in NumPy, and the function achieves it using nested loops.\n",
        "\n",
        "In the provided naive_add_matrix_and_vector function, the variables x and y are not related to the common usage of x (input features) and y (labels) in machine learning contexts.\n",
        "\n",
        "Instead, here:\n",
        "\n",
        "x is a matrix (a 2D array), which could represent any data structured in rows and columns (for example, features or data points).\n",
        "\n",
        "y is a vector (a 1D array), which could represent any set of values that need to be added to each row of the matrix x.\n",
        "\n",
        "##Key Points:\n",
        "The purpose of this function is to add the vector y to each row of the matrix x.\n",
        "\n",
        "This operation is commonly known as broadcasting in NumPy, and in neural networks, such an operation could resemble adding bias terms to every data point in a batch (though y is not strictly a bias here).\n",
        "\n",
        "There is no notion of \"labels\" involved in this function. The y in this context is simply a vector being added to the matrix.\n",
        "\n",
        "##Clarification of Usage:\n",
        "If x were features (e.g., pixel intensities of an image or sensor measurements), the vector y could represent a set of corrections, biases, or a transformation you want to apply to each feature across multiple data points (rows).\n",
        "\n",
        "In the context of neural networks or matrix operations, these names don't correspond to input (x) and target labels (y).\n",
        "\n",
        "They are just generic names for matrix and vector data being added element-wise.\n",
        "\n",
        "##In Terms of ReLU:\n",
        "If you were referring to the earlier use of ReLU, the operation here doesn’t directly involve the ReLU function.\n",
        "\n",
        "It is merely adding a vector y to each row of a matrix x. The ReLU function, on the other hand, sets negative values to zero.\n",
        "\n",
        "If the function were used in the context of neural networks, x might be an activation matrix from a layer of a neural network, and adding a vector y could represent adding bias terms to the activations before applying the ReLU operation.\n",
        "\n",
        "Would you like to clarify how this fits into the broader context you're working with, like neural networks or matrix manipulations?\n",
        "\n",
        "--------------------------\n",
        "## Assertions:\n",
        "\n",
        "    assert len(x.shape) == 2\n",
        "    assert len(y.shape) == 1\n",
        "    assert x.shape[1] == y.shape[0]\n",
        "\n",
        "\n",
        "These assertions check that:\n",
        "\n",
        "x is a 2D array (matrix).\n",
        "\n",
        "y is a 1D array (vector).\n",
        "\n",
        "The number of columns in x (x.shape[1]) matches the number of elements in y (y.shape[0]), ensuring that the vector can be added to each row of the matrix.\n",
        "\n",
        "--------------------------------\n",
        "##Copying the Input Matrix:\n",
        "\n",
        "    x = x.copy()\n",
        "\n",
        "This creates a copy of the matrix x so that the original matrix is not modified directly.\n",
        "\n",
        "##Nested Loops to Add Vector to Matrix:\n",
        "\n",
        "    for i in range(x.shape[0]):\n",
        "        for j in range(x.shape[1]):\n",
        "            x[i, j] += y[j]\n",
        "\n",
        "The outer loop for i in range(x.shape[0]) iterates over the rows of the matrix x (the number of rows is x.shape[0]).\n",
        "\n",
        "The inner loop for j in range(x.shape[1]) iterates over the columns of the matrix x (the number of columns is x.shape[1]).\n",
        "\n",
        "Inside the inner loop, the function adds the corresponding element from the vector y to the element in the matrix x.\n",
        "\n",
        "Specifically, for each row i and column j, the value x[i, j] is incremented by y[j].\n",
        "\n",
        "-------------------------------------\n",
        "#Return the Modified Matrix:\n",
        "\n",
        "    return x\n",
        "\n",
        "After applying the vector to each row of the matrix, the function returns the modified matrix x.\n",
        "\n",
        "--------------------\n",
        "#Example:\n",
        "Suppose x is a 2x3 matrix:\n",
        "\n",
        "    x = np.array([[1, 2, 3],\n",
        "                  [4, 5, 6]])\n",
        "\n",
        "And y is a 1D vector of length 3:\n",
        "\n",
        "    y = np.array([10, 20, 30])\n",
        "\n",
        "Calling naive_add_matrix_and_vector(x, y) will result in:\n",
        "\n",
        "First, the vector [10, 20, 30] is added to the first row: [1 + 10, 2 + 20, 3 + 30] = [11, 22, 33].\n",
        "\n",
        "Then, the vector is added to the second row: [4 + 10, 5 + 20, 6 + 30] = [14, 25, 36].\n",
        "\n",
        "The output will be:\n",
        "\n",
        "    array([[11, 22, 33],\n",
        "           [14, 25, 36]])\n",
        "\n",
        "------------------------------\n",
        "#Broadcasting in NumPy\n",
        "In NumPy, this operation is known as broadcasting, where NumPy automatically adds the vector to each row of the matrix without explicit loops.\n",
        "\n",
        "You can achieve the same operation more efficiently with NumPy like this:\n",
        "\n",
        "    z = x + y\n",
        "\n",
        "This automatically applies the vector y to each row of x, thanks to NumPy's broadcasting rules.\n",
        "\n",
        "--------------------------\n",
        "#Summary\n",
        "The function naive_add_matrix_and_vector manually adds a vector to each row of a matrix using nested loops.\n",
        "\n",
        "It checks for valid input shapes and then performs the addition element-wise.\n",
        "\n",
        "This operation can be more efficiently performed in NumPy using broadcasting (x + y)."
      ],
      "metadata": {
        "id": "FjK8ZgAyrlwf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "x = np.random.random((64, 3, 32, 10))\n",
        "y = np.random.random((32, 10))\n",
        "z = np.maximum(x, y)"
      ],
      "metadata": {
        "id": "kgTl5kZvvyP5"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Broadcasting Example 2\n",
        "This snippet demonstrates an example of broadcasting in NumPy, where operations between arrays of different shapes are performed without explicit looping.\n",
        "\n",
        "--------------------------\n",
        "Let’s break down each part of the code to understand how it works.\n",
        "\n",
        "##Creating Random Arrays:\n",
        "\n",
        "    x = np.random.random((64, 3, 32, 10))\n",
        "\n",
        "##x is a 4D array with shape (64, 3, 32, 10).\n",
        "\n",
        "This means that x contains 64 samples, each with 3 channels (or features), where each channel has a matrix of shape (32, 10).\n",
        "\n",
        "The values in x are random floats between 0 and 1.\n",
        "\n",
        "    y = np.random.random((32, 10))\n",
        "\n",
        "y is a 2D array with shape (32, 10).\n",
        "\n",
        "This is a matrix with 32 rows and 10 columns, filled with random floats between 0 and 1.\n",
        "\n",
        "------------------------\n",
        "#Applying the np.maximum Function:\n",
        "\n",
        "    z = np.maximum(x, y)\n",
        "\n",
        "np.maximum performs an element-wise maximum between the arrays x and y.\n",
        "\n",
        "For each element in x and y, it compares the values and returns the larger of the two.\n",
        "\n",
        "This works due to broadcasting in NumPy.\n",
        "\n",
        "----------------------\n",
        "#Broadcasting Explained:\n",
        "Broadcasting in NumPy allows arrays of different shapes to be used in arithmetic operations as long as the shapes are compatible.\n",
        "\n",
        "NumPy tries to expand smaller arrays along dimensions to match the larger array’s shape.\n",
        "\n",
        "---------------------------\n",
        "#Broadcasting Rules:\n",
        "Align the shapes of the arrays starting from the rightmost dimension.\n",
        "\n",
        "---------------------------\n",
        "#If dimensions are not equal:\n",
        "A dimension of size 1 can be stretched to match the larger dimension.\n",
        "\n",
        "Dimensions that are not 1 and don’t match will result in an error.\n",
        "\n",
        "--------------------------\n",
        "#Shape Alignment:\n",
        "    x has shape (64, 3, 32, 10).\n",
        "    y has shape (32, 10).\n",
        "\n",
        "To align these shapes:\n",
        "\n",
        "The rightmost two dimensions of x and y both have the shape (32, 10), so they are directly compatible for element-wise comparison.\n",
        "\n",
        "NumPy automatically broadcasts y across the first two dimensions of x.\n",
        "\n",
        "This means y is virtually \"expanded\" to match the first two dimensions of x, replicating the data in y for each sample and channel in x.\n",
        "\n",
        "In this case, y is repeated 64 times (for the first dimension) and 3 times (for the second dimension).\n",
        "\n",
        "--------------------------\n",
        "#Final Shape:\n",
        "The resulting array z will have the same shape as x, which is (64, 3, 32, 10), since the maximum operation is applied element-wise across the broadcasted arrays.\n",
        "\n",
        "--------------------\n",
        "#Example of the Operation:\n",
        "Suppose an element in x at position (0, 0, 0, 0) is 0.5, and the corresponding element in y at position (0, 0) is 0.7.\n",
        "\n",
        "The np.maximum function will compare these two values and choose the larger one, so in this case, z[0, 0, 0, 0] will be 0.7.\n",
        "\n",
        "##Key Points:\n",
        "x is a 4D array, and y is a 2D array.\n",
        "\n",
        "Due to broadcasting, y is replicated across the first two dimensions of x to match the shapes, and then an element-wise maximum is computed.\n",
        "\n",
        "The result z has the same shape as x.\n",
        "\n",
        "--------------------\n",
        "#Summary\n",
        "x is a 4D tensor, and y is a 2D tensor.\n",
        "\n",
        "Broadcasting allows NumPy to perform an element-wise maximum operation between x and y by aligning their shapes.\n",
        "\n",
        "The resulting tensor z has the shape (64, 3, 32, 10)."
      ],
      "metadata": {
        "id": "MK5YOpUCwTHp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tensor product"
      ],
      "metadata": {
        "id": "e3V_AUeN0qFn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.random.random((32,))\n",
        "y = np.random.random((32,))\n",
        "z = np.dot(x, y)"
      ],
      "metadata": {
        "id": "p84kHM5j0sgK"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def naive_vector_dot(x, y):\n",
        "    assert len(x.shape) == 1\n",
        "    assert len(y.shape) == 1\n",
        "    assert x.shape[0] == y.shape[0]\n",
        "    z = 0.\n",
        "    for i in range(x.shape[0]):\n",
        "        z += x[i] * y[i]\n",
        "    return z"
      ],
      "metadata": {
        "id": "uXu1dTND0yGq"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dot product\n",
        "The function naive_vector_dot implements a dot product (or inner product) between two vectors x and y manually, using a loop.\n",
        "\n",
        "This is a fundamental operation in linear algebra, particularly in machine learning, where it is often used in tasks such as computing the weighted sum of features in neural networks.\n",
        "\n",
        "----------------------\n",
        "Assertions:\n",
        "\n",
        "    assert len(x.shape) == 1\n",
        "    assert len(y.shape) == 1\n",
        "    assert x.shape[0] == y.shape[0]\n",
        "\n",
        "These checks ensure:\n",
        "\n",
        "x is a 1D vector (it has one dimension).\n",
        "\n",
        "y is a 1D vector.\n",
        "\n",
        "Both vectors have the same length (the number of elements in x and y must match for the dot product to be valid).\n",
        "\n",
        "------------------------------\n",
        "Initialize the Dot Product:\n",
        "\n",
        "    z = 0\n",
        "\n",
        "The result of the dot product will be stored in z, which starts at 0.\n",
        "\n",
        "-------------------------\n",
        "#Loop Through the Elements of x and y:\n",
        "\n",
        "    for i in range(x.shape[0]):\n",
        "       z += x[i] * y[i]\n",
        "\n",
        "The loop iterates over each element in the vectors. x.shape[0] gives the length of the vectors.\n",
        "\n",
        "For each element i, it multiplies the corresponding elements from x and y (x[i] * y[i]), and adds the result to z.\n",
        "\n",
        "-------------------------------\n",
        "#Return the Dot Product:\n",
        "    return z\n",
        "\n",
        "After the loop finishes, the function returns z, which now holds the sum of all element-wise products of x and y.\n",
        "\n",
        "-------------------------\n",
        "#Example\n",
        "Suppose we have two vectors:\n",
        "\n",
        "    x = np.array([1, 2, 3])\n",
        "    y = np.array([4, 5, 6])\n",
        "\n",
        "The dot product is calculated as:\n",
        "\n",
        "    1*4 + 2*5 + 3*6 = 4 + 10 + 18 = 32\n",
        "\n",
        "So, naive_vector_dot(x, y) would return 32.\n",
        "\n",
        "-----------------------------\n",
        "#Explanation of the Dot Product:\n",
        "The dot product of two vectors is the sum of the products of their corresponding elements. Mathematically, for two vectors    \n",
        "    x = [x1, x2, ..., xn] and y = [y1, y2, ..., yn]\n",
        "    \n",
        "the dot product is:\n",
        "\n",
        "    x · y = x1*y1 + x2*y2 + ... + xn*yn\n",
        "\n",
        "---------------------------\n",
        "##In NumPy\n",
        "\n",
        "You could achieve the same operation more efficiently in NumPy using:\n",
        "\n",
        "    z = np.dot(x, y)\n",
        "\n",
        "This function does the same thing but is much faster because it's optimized for performance.\n",
        "\n",
        "-------------------------------\n",
        "#Summary\n",
        "This function calculates the dot product between two 1D vectors manually, using a loop.\n",
        "\n",
        "It checks that the vectors are of the same length and returns the sum of the element-wise products.\n",
        "\n",
        "The dot product is a key operation in many machine learning algorithms, such as computing the weighted sum of features or activations in neural networks."
      ],
      "metadata": {
        "id": "8Y59KaNy1zDP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def naive_matrix_vector_dot(x, y):\n",
        "    assert len(x.shape) == 2\n",
        "    assert len(y.shape) == 1\n",
        "    assert x.shape[1] == y.shape[0]\n",
        "    z = np.zeros(x.shape[0])\n",
        "    for i in range(x.shape[0]):\n",
        "        for j in range(x.shape[1]):\n",
        "            z[i] += x[i, j] * y[j]\n",
        "    return z"
      ],
      "metadata": {
        "id": "LEwNcVVsIsjk"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Naive_matrix_vector_dot multiplication\n",
        "The function naive_matrix_vector_dot performs the matrix-vector multiplication between a matrix x and a vector y.\n",
        "\n",
        "This operation is fundamental in linear algebra and is widely used in machine learning tasks like computing the activations in a neural network layer.\n",
        "\n",
        "-------------------------------\n",
        "##Assertions:\n",
        "\n",
        "    assert len(x.shape) == 2\n",
        "    assert len(y.shape) == 1\n",
        "    assert x.shape[1] == y.shape[0]\n",
        "\n",
        "These checks ensure:\n",
        "\n",
        "x is a 2D matrix (it has two dimensions, i.e., rows and columns).\n",
        "\n",
        "y is a 1D vector.\n",
        "\n",
        "The number of columns in the matrix x (i.e., x.shape[1]) must match the number of elements in the vector y (i.e., y.shape[0]).\n",
        "\n",
        "This is necessary for matrix-vector multiplication to be valid.\n",
        "\n",
        "-----------------------------\n",
        "#Initialize the Result Vector:\n",
        "\n",
        "    z = np.zeros(x.shape[0])\n",
        "\n",
        "z is initialized as a vector of zeros with the same length as the number of rows in x (i.e., x.shape[0]).\n",
        "\n",
        "This will hold the result of the matrix-vector multiplication.\n",
        "Loop Through the Rows and Columns:\n",
        "\n",
        "    for i in range(x.shape[0]):\n",
        "       for j in range(x.shape[1]):\n",
        "          z[i] += x[i, j] * y[j]\n",
        "\n",
        "The outer loop iterates over the rows of x (i runs over x.shape[0]).\n",
        "\n",
        "The inner loop iterates over the columns of x (j runs over x.shape[1]).\n",
        "\n",
        "For each element in row i of matrix x, it multiplies x[i, j] (the element at row i, column j) with y[j] (the corresponding element in vector y) and adds the result to z[i].\n",
        "\n",
        "Essentially, this is calculating the dot product between row i of matrix x and vector y.\n",
        "\n",
        "--------------------------\n",
        "#Return the Result:\n",
        "\n",
        "    return z\n",
        "\n",
        "After all the rows and columns have been processed, the function returns the result vector z, which contains the matrix-vector product.\n",
        "\n",
        "-----------------------\n",
        "#Example\n",
        "Let's consider an example with a matrix x and a vector y:\n",
        "\n",
        "    x = np.array([[1, 2, 3], [4, 5, 6]])  # Shape: (2, 3)\n",
        "    y = np.array([1, 2, 3])               # Shape: (3,)\n",
        "\n",
        "Matrix x has 2 rows and 3 columns, and vector y has 3 elements.\n",
        "\n",
        "The function computes the dot product between each row of x and y:\n",
        "\n",
        "For the first row of x ([1, 2, 3]) and y ([1, 2, 3]):\n",
        "\n",
        "    1*1 + 2*2 + 3*3 = 1 + 4 + 9 = 14\n",
        "\n",
        "For the second row of x ([4, 5, 6]) and y ([1, 2, 3]):\n",
        "\n",
        "    4*1 + 5*2 + 6*3 = 4 + 10 + 18 = 32\n",
        "\n",
        "\n",
        "##So, the result vector z will be:\n",
        "\n",
        "    z = np.array([14, 32])  # Shape: (2,)\n",
        "\n",
        "------------------------------\n",
        "#Matrix-Vector Multiplication:\n",
        "Matrix-vector multiplication is a key operation where, given a matrix x of shape (m, n) and a vector y of shape (n,), the result is a vector z of shape (m,).\n",
        "\n",
        "Each element of z is the dot product of the corresponding row of x with the vector y.\n",
        "\n",
        "#In NumPy:\n",
        "You could achieve the same operation more efficiently using:\n",
        "\n",
        "    z = np.dot(x, y)\n",
        "\n",
        "This performs matrix-vector multiplication in a highly optimized manner.\n",
        "\n",
        "-----------------------\n",
        "#Summary\n",
        "The function naive_matrix_vector_dot multiplies a matrix x (2D) by a vector y (1D) manually, using nested loops.\n",
        "\n",
        "It computes the dot product of each row in the matrix with the vector and stores the result in a new vector z.\n",
        "\n",
        "Matrix-vector multiplication is crucial in various linear algebra and machine learning tasks."
      ],
      "metadata": {
        "id": "5245mKACJF0Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def naive_matrix_vector_dot(x, y):\n",
        "    z = np.zeros(x.shape[0])\n",
        "    for i in range(x.shape[0]):\n",
        "        z[i] = naive_vector_dot(x[i, :], y)\n",
        "    return z"
      ],
      "metadata": {
        "id": "d1XB_Z12N4Wt"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def naive_matrix_dot(x, y):\n",
        "    assert len(x.shape) == 2\n",
        "    assert len(y.shape) == 2\n",
        "    assert x.shape[1] == y.shape[0]\n",
        "    z = np.zeros((x.shape[0], y.shape[1]))\n",
        "    for i in range(x.shape[0]):\n",
        "        for j in range(y.shape[1]):\n",
        "            row_x = x[i, :]\n",
        "            column_y = y[:, j]\n",
        "            z[i, j] = naive_vector_dot(row_x, column_y)\n",
        "    return z"
      ],
      "metadata": {
        "id": "1AoBOgkNN71s"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tensor reshaping"
      ],
      "metadata": {
        "id": "dZsOC9-lOfJa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PL0yQ9ZpQGVr",
        "outputId": "07ed2bf9-8d13-48d2-be70-3e072d0937c4"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_images = train_images.reshape((60000, 28 * 28))"
      ],
      "metadata": {
        "id": "OYFhusW8OiuT"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x=np.array([[0., 1.],\n",
        "            [2., 3.],\n",
        "            [4., 5.0]])\n",
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OUESfH-nQhzp",
        "outputId": "26fd677e-c9c1-4e4e-98c0-f6d949e1ea4a"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = x.reshape(6, 1)\n",
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g3oOeX2MRz_L",
        "outputId": "ea455f9d-ee62-4c0c-f960-fb2c9077e6cd"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.],\n",
              "       [1.],\n",
              "       [2.],\n",
              "       [3.],\n",
              "       [4.],\n",
              "       [5.]])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Matrices (rank-2 tensors)"
      ],
      "metadata": {
        "id": "IlALRVYeCuu2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.array([[5, 78, 2, 34, 0],\n",
        "              [6, 79, 3, 35, 1],\n",
        "              [7, 80, 4, 36, 2]])\n",
        "x.ndim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4vcifHBkC0KT",
        "outputId": "d3777aed-ddaf-4c68-ce91-d96f30b13bd5"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x= np.array([[5, 78, 2, 34, 0],\n",
        "            [6, 79, 3, 35, 1],\n",
        "            [7, 80, 4, 36, 2]])\n",
        "x.ndim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-TdOguoxDUFJ",
        "outputId": "1cedbb74-9c44-450c-8746-871d182c1a61"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Rank-3 and higher-rank tensors"
      ],
      "metadata": {
        "id": "8chkkG9JC6Pv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.array([[[5, 78, 2, 34, 0],\n",
        "               [6, 79, 3, 35, 1],\n",
        "               [7, 80, 4, 36, 2]],\n",
        "              [[5, 78, 2, 34, 0],\n",
        "               [6, 79, 3, 35, 1],\n",
        "               [7, 80, 4, 36, 2]],\n",
        "              [[5, 78, 2, 34, 0],\n",
        "               [6, 79, 3, 35, 1],\n",
        "               [7, 80, 4, 36, 2]]])\n",
        "x.ndim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AYRJpdpwHEOD",
        "outputId": "60c42167-0123-496d-94ff-9c61b802e65f"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Key attributes"
      ],
      "metadata": {
        "id": "SZ8D_zMgHi0H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
      ],
      "metadata": {
        "id": "QRGjcdkCHm0T"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_images.ndim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oszzi7uUJMpP",
        "outputId": "0518bd39-a738-4830-b19a-c494f45d76e5"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels.ndim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7k1TF84JTA1",
        "outputId": "764b7232-75c9-4370-cc8d-f8b5f7216308"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_images.dtype"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AtwhHFL0LPu5",
        "outputId": "d0c0dcee-27b9-417a-a755-739b995076a2"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dtype('uint8')"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Displaying the fourth digit"
      ],
      "metadata": {
        "id": "dFW9shNPNwRB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "digit = train_images[4]\n",
        "plt.imshow(digit, cmap=plt.cm.binary)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R4Ws2BWvNzq2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Changing 'train_image' to 'train_images' assuming this is the intended variable\n",
        "digit = train_images[4]\n",
        "plt.imshow(digit, cmap=plt.cm.binary)\n",
        "plt.show()"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "jzNNN4xgOt5A",
        "outputId": "cea26f6e-fa70-4868-dd15-65ec77c21a68"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The engine of neural networks: gradient-based optimization"
      ],
      "metadata": {
        "id": "hUQkCbR-qdSj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "x = tf.Variable(0.)\n",
        "with tf.GradientTape() as tape:\n",
        "    y = 2 * x + 3\n",
        "grad_of_y_wrt_x = tape.gradient(y, x)"
      ],
      "metadata": {
        "id": "cqk8i7cGqc1X"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Automatic differentiation mechanism using tf.GradientTape\n",
        "\n",
        "Here’s an explanation of what’s happening step-by-step:\n",
        "\n",
        "-----------------------\n",
        "# 1. x = tf.Variable(0.)\n",
        "You create a TensorFlow variable x initialized to 0.0.\n",
        "\n",
        "The tf.Variable is a special type of tensor that allows for mutation, meaning its value can be updated during training or other operations.\n",
        "\n",
        "--------------------------------\n",
        "#2. with tf.GradientTape() as tape:\n",
        "The tf.GradientTape context is used to record operations for automatic differentiation.\n",
        "\n",
        "Inside this context, TensorFlow keeps track of any operation that involves trainable variables like x.\n",
        "\n",
        "These recorded operations allow TensorFlow to compute the gradient of the output with respect to any trainable variable inside the context.\n",
        "\n",
        "----------------------------\n",
        "#3. y = 2 * x + 3\n",
        "This defines the operation you're performing on x. Here, y is a function of x:\n",
        "\n",
        "$$y=2x+3$$\n",
        "\n",
        "This is a simple linear equation.\n",
        "\n",
        "--------------------------\n",
        "#4. grad_of_y_wrt_x = tape.gradient(y, x)\n",
        "After the context block (with), TensorFlow computes the gradient of y with respect to x using tape.gradient.\n",
        "\n",
        "The derivative (or gradient) of\n",
        "\n",
        "$$y=2x+3$$\n",
        "\n",
        "with respect to 𝑥, x is simply 2 because:\n",
        "\n",
        "$$𝑑\n",
        "𝑦/\n",
        "𝑑\n",
        "𝑥\n",
        "=\n",
        "2\n",
        "dx/\n",
        "dy\n",
        "​=2$$\n",
        "\n",
        "So, the value of grad_of_y_wrt_x will be 2.0.\n",
        "\n",
        "Full Explanation of Gradient Computation:\n",
        "\n",
        "--------------------------\n",
        "#Automatic Differentiation:\n",
        "\n",
        "TensorFlow's tf.GradientTape is used for automatic differentiation, allowing you to compute gradients of scalar functions with respect to their inputs (variables like x).\n",
        "\n",
        "Gradient of y with respect to x: In this case, the gradient of\n",
        "\n",
        "$$y=2x+3$$\n",
        "\n",
        "with respect to 𝑥 is constant, equal to 2, because the slope of this linear function is always 2.\n",
        "\n",
        "-----------------------\n",
        "#Output\n",
        "\n",
        "print(grad_of_y_wrt_x)  # Will print 2.0\n",
        "\n",
        "This code would output 2.0 as the gradient of 𝑦 with respect to 𝑥."
      ],
      "metadata": {
        "id": "ElESa-uQqv1C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = tf.Variable(tf.random.uniform((2, 2)))\n",
        "with tf.GradientTape() as tape:\n",
        "    y = 2 * x + 3\n",
        "grad_of_y_wrt_x = tape.gradient(y, x)"
      ],
      "metadata": {
        "id": "s83OmR6g3ncM"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Performing automatic differentiation in TensorFlow on a 2x2 matrix (tensor) using tf.GradientTape.\n",
        "\n",
        "Let's break down what’s happening step by step:\n",
        "\n",
        "---------------------------\n",
        "# 1. x = tf.Variable(tf.random.uniform((2, 2)))\n",
        "You create a 2x2 matrix x as a TensorFlow variable.\n",
        "\n",
        "The matrix is filled with random values from a uniform distribution between 0 and 1.\n",
        "\n",
        "Since you use tf.Variable, this tensor is trainable, and TensorFlow can track operations performed on it for gradient computation.\n",
        "\n",
        "--------------------------------------\n",
        "# 2. with tf.GradientTape() as tape:\n",
        "This block records operations on x so that you can later compute the gradient of y with respect to x.\n",
        "\n",
        "---------------------------------------\n",
        "#3. y = 2 * x + 3\n",
        "This operation is performed element-wise on x. Each element of x is multiplied by 2, and then 3 is added to it.\n",
        "\n",
        "Since x is a 2x2 matrix, y will also be a 2x2 matrix, where each element follows the equation:\n",
        "\n",
        "$$y=2x+3$$\n",
        "\n",
        "--------------------------------\n",
        "#4. grad_of_y_wrt_x = tape.gradient(y, x)\n",
        "Here, you're computing the gradient of y with respect to x. Since\n",
        "\n",
        "$$y=2x+3$$\n",
        "\n",
        "is a linear function of 𝑥, the derivative (gradient) of\n",
        "𝑦 with respect to 𝑥, x is just 2 for every element of x.\n",
        "\n",
        "This is because:\n",
        "\n",
        "$$𝑑\n",
        "𝑦/\n",
        "𝑑\n",
        "𝑥\n",
        "=\n",
        "2\n",
        "dx/\n",
        "dy\n",
        "​\n",
        " =2$$\n",
        "\n",
        "So, the gradient will be a 2x2 matrix where each entry is 2.\n",
        "\n",
        "------------------------\n",
        "#Example\n",
        "Suppose x is randomly generated as:\n",
        "\n",
        "    x = [[0.5, 0.7],\n",
        "         [0.9, 0.2]]\n",
        "\n",
        "Then, y would be:\n",
        "\n",
        "    y = [[2 * 0.5 + 3, 2 * 0.7 + 3],\n",
        "         [2 * 0.9 + 3, 2 * 0.2 + 3]]\n",
        "       = [[4.0, 4.4],\n",
        "         [4.8, 3.4]]\n",
        "The gradient of y with respect to x is:\n",
        "\n",
        "    grad_of_y_wrt_x = [[2, 2],\n",
        "                       [2, 2]]\n",
        "\n",
        "------------------------------------                       \n",
        "# Output:\n",
        "\n",
        "    print(grad_of_y_wrt_x)  # Will print a 2x2 matrix of 2s\n",
        "This code will output:\n",
        "\n",
        "    <tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
        "     array([[2., 2.],\n",
        "           [2., 2.]], dtype=float32)>\n",
        "\n",
        "This happens because the derivative of\n",
        "\n",
        "$$y=2x+3$$\n",
        "\n",
        "with respect to each element of x is always 2."
      ],
      "metadata": {
        "id": "1Uc6BsXN32I6"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMAVAHcogUoo1tdplwYAVxA",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}