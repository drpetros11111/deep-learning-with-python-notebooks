{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/drpetros11111/deep-learning-with-python-notebooks/blob/pp_study/02_DataRepr_tf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSAmxRwICQ2K"
      },
      "source": [
        "# Data representations for neural networks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rQufhR1sDFaV"
      },
      "outputs": [],
      "source": [
        "# TensorFlow for Linear Algebra and Matrix Operations\n",
        "import tensorflow as tf\n",
        "from tensorflow.linalg import inv, det, eig, svd, norm, cholesky, solve, eye, diag\n",
        "from tensorflow import matmul, reshape, transpose, reduce_sum\n",
        "\n",
        "# Matplotlib for plotting\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Optional: NumPy for array handling (if needed for some use cases)\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4r4D6_ACXy8"
      },
      "source": [
        "## Scalars (rank-0 tensors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5K-oTnU7CB5x",
        "outputId": "18570631-b627-4d70-f1a0-8215120f50e4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(12)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import numpy as np\n",
        "x = np.array(12)\n",
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5uxc7JVoDjeX",
        "outputId": "827d7323-a220-4160-9dbe-4ca6aa9e43a6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "x.ndim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqyleAgSD5AD"
      },
      "source": [
        "## Vectors (rank-1 tensors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "sZyjHAh8DyLt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb310f60-6cb6-401e-d2aa-4b2f4871e103"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([12,  3,  6, 14,  7])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "x = np.array([12, 3, 6, 14, 7])\n",
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0McyaQdJEGxo",
        "outputId": "1b2ba771-e9d5-49e9-c13f-f5ddf7b123d4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "x =np.array([12, 3, 6, 14, 7 ])\n",
        "x\n",
        "x.ndim\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GMdwON6FZED"
      },
      "source": [
        "# Syntax of a function call\n",
        "In the expression x = np.array([12, 3, 6, 14, 7]), both parentheses and brackets serve specific purposes:\n",
        "\n",
        "------------------------------------\n",
        "##Breakdown of the Syntax:\n",
        "    np.array(...)\n",
        "\n",
        "##Parentheses (...):\n",
        "\n",
        "The function np.array() is called with its argument(s) inside parentheses.\n",
        "\n",
        "In Python, parentheses are used to call functions and pass arguments to them.\n",
        "\n",
        "Here, you are calling the array function from the numpy library (aliased as np), which converts a list (or other array-like structures) into a NumPy array.\n",
        "\n",
        "     [12, 3, 6, 14, 7]:\n",
        "\n",
        "##Brackets [...]:\n",
        "\n",
        "The square brackets are used to create a list in Python. In this case, [12, 3, 6, 14, 7] is a list of integers.\n",
        "\n",
        "This list is passed as an argument to the np.array() function, which converts it into a NumPy array.\n",
        "\n",
        "------------------------------\n",
        "#Summary\n",
        "Function Call: The parentheses are necessary for calling the np.array function.\n",
        "\n",
        "List Creation: The brackets are used to define the list of values that you want to convert into an array."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ew77wA9rGs09"
      },
      "source": [
        "# Matrices (rank-2 tensors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S4SetywsGx4t",
        "outputId": "a1f8e7fc-9b66-4e9d-f3f2-139869d8f1ae"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "x = np.array([[5, 78, 2, 34, 0],\n",
        "              [6, 79, 3, 35, 1],\n",
        "              [7, 80, 4, 36, 2]])\n",
        "x.ndim\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eH3hR0GVHWX8",
        "outputId": "06809c2e-ee5d-4c62-e8e1-e13fa1b53e9b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3, 5)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UgfmG-buHfUU",
        "outputId": "b9d9dc55-e619-4721-ee34-37a6c02428ee"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "y = np.array([[1, 2, 3, 4, 5],\n",
        "              [6, 7, 8, 9, 99],\n",
        "              [7, 77, 9, 88, 11]])\n",
        "y.ndim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KfjTqoZWIJTp",
        "outputId": "37482c13-ba0d-4420-8b16-18d639310893"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3, 5)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "y.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUemhwCFINc4"
      },
      "source": [
        "# Rank-3 and higher-rank tensors"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The gears of neural networks: tensor operations"
      ],
      "metadata": {
        "id": "9d9fbNqlbaA9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Element-wise operations"
      ],
      "metadata": {
        "id": "Qi1dOg5CbhFJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "g3ksQvAWIV0b"
      },
      "outputs": [],
      "source": [
        "def naive_relu(x):\n",
        "    assert len(x.shape) == 2\n",
        "    x = x.copy()\n",
        "    for i in range(x.shape[0]):\n",
        "        for j in range(x.shape[1]):\n",
        "            x[i, j] = max(x[i, j], 0)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Naive Relu\n",
        "This function, naive_relu, is an implementation of the ReLU (Rectified Linear Unit) activation function, applied manually (or \"naively\") to a 2D array (matrix) x.\n",
        "\n",
        "-------------------------------------\n",
        "--------------------------------------\n",
        "# Explanation of the Function\n",
        "\n",
        "##Input Validation:\n",
        "    assert len(x.shape) == 2\n",
        "\n",
        "\n",
        "This assert statement checks that the input x is a 2D array (has exactly 2 dimensions).\n",
        "\n",
        "If x doesn't have 2 dimensions, the function will raise an assertion error.\n",
        "\n",
        "----------------------------------------\n",
        "\n",
        "##Copying the Input:\n",
        "    x = x.copy()\n",
        "\n",
        "A copy of the input x is made to avoid modifying the original array (this is important for preserving the input when working with arrays in-place).\n",
        "\n",
        "-------------------------------------\n",
        "##Iterating Over the Array:\n",
        "\n",
        "    for i in range(x.shape[0]):\n",
        "       for j in range(x.shape[1]):\n",
        "           x[i, j] = max(x[i, j], 0)\n",
        "\n",
        "The function uses two nested loops to iterate over every element in the 2D array x.\n",
        "\n",
        "x.shape[0] refers to the number of rows.\n",
        "\n",
        "x.shape[1] refers to the number of columns.\n",
        "\n",
        "For each element x[i, j], the ReLU operation is applied:\n",
        "x[i, j] = max(x[i, j], 0) replaces each element with the maximum of the value itself or 0.\n",
        "\n",
        "In other words, if the element is negative, it becomes 0 (ReLU operation).\n",
        "\n",
        "------------------------\n",
        "#Return the Result\n",
        "\n",
        "    return x\n",
        "\n",
        "The modified array x is returned as output after applying the ReLU operation to all its elements.\n",
        "\n",
        "------------------------\n",
        "#What is ReLU?\n",
        "ReLU is a common activation function used in neural networks. It stands for Rectified Linear Unit.\n",
        "Mathematically, ReLU is defined as:\n",
        "\n",
        "$$ReLU(x)=max(0,x)$$\n",
        "\n",
        "If the input is positive, it returns the input.\n",
        "\n",
        "If the input is negative, it returns 0.\n",
        "\n",
        "--------------------------------\n",
        "#Example\n",
        "If x is a 2x2 matrix:\n",
        "\n",
        "    x = np.array([[1, -2], [-3, 4]])\n",
        "\n",
        "The naive_relu(x) function will:\n",
        "\n",
        "Loop through each element.\n",
        "\n",
        "Apply the ReLU operation (replace negative values with 0).\n",
        "\n",
        "-----------------------\n",
        "#Return:\n",
        "    array([[1, 0],\n",
        "           [0, 4]])\n",
        "\n",
        "----------------------------\n",
        "#Summary\n",
        "The naive_relu function manually implements the ReLU activation function for a 2D array using a nested loop.\n",
        "\n",
        "It sets any negative values in the input matrix to 0 while keeping positive values unchanged.\n",
        "\n",
        "This is a basic, non-optimized implementation, and libraries like TensorFlow or NumPy provide much more efficient ways to apply ReLU on large arrays.\n",
        "\n",
        "Would you like to see a more optimized version?"
      ],
      "metadata": {
        "id": "yhdZUD7Ucb2p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def naive_add(x, y):\n",
        "    assert len(x.shape) == 2\n",
        "    assert x.shape == y.shape\n",
        "    x = x.copy()\n",
        "    for i in range(x.shape[0]):\n",
        "        for j in range(x.shape[1]):\n",
        "            x[i, j] += y[i, j]\n",
        "    return x"
      ],
      "metadata": {
        "id": "G0iQUKp3iumf"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "x = np.random.random((20, 100))\n",
        "y = np.random.random((20, 100))\n",
        "\n",
        "t0 = time.time()\n",
        "for _ in range(1000):\n",
        "    z = x + y\n",
        "    z = np.maximum(z, 0.)\n",
        "print(\"Took: {0:.2f} s\".format(time.time() - t0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8OACluf9izwK",
        "outputId": "ab546029-0a44-4ecf-df0d-6a04bbde2f7f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Took: 0.01 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "x = np.random.random((20, 100))\n",
        "y = np.random.random((20, 100))\n",
        "\n",
        "t0 = time.time()\n",
        "for _ in range(1000):\n",
        "    z = x + y\n",
        "    z = np.maximum(z, 0.)\n",
        "print(\"Took: {0:.2f} s\".format(time.time() - t0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fN_uVFRRjXXe",
        "outputId": "44499919-026a-4e0e-8470-d2e592f6cc9e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Took: 0.03 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Benchmark Relu\n",
        "This code snippet benchmarks how long it takes to perform a vectorized operation on two 2D NumPy arrays (matrices) 1,000 times.\n",
        "\n",
        "-----------------------------\n",
        "##Importing the time Module:\n",
        "\n",
        "    import time\n",
        "\n",
        "The time module is used to measure the time it takes to perform certain operations (in this case, to benchmark the speed of adding two arrays and applying the ReLU-like operation).\n",
        "\n",
        "-----------------------------------\n",
        "## Creating Random Arrays:\n",
        "\n",
        "    x = np.random.random((20, 100))\n",
        "    y = np.random.random((20, 100))\n",
        "\n",
        "Two random arrays, x and y, are created using NumPy's random.random function.\n",
        "\n",
        "Both arrays have the shape (20, 100), meaning they are 2D arrays with 20 rows and 100 columns.\n",
        "\n",
        "Each element in x and y is a random float between 0 and 1.\n",
        "\n",
        "-----------------------------\n",
        "##Timing the Operation:\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "t0 stores the current time at the start of the benchmark using time.time().\n",
        "\n",
        "-----------------------------------\n",
        "##Performing the Operation 1,000 Times:\n",
        "\n",
        "    for _ in range(1000):\n",
        "       z = x + y\n",
        "       z = np.maximum(z, 0.)\n",
        "\n",
        "A loop runs 1,000 times. Inside the loop:\n",
        "Addition: z = x + y adds the two arrays x and y element-wise.\n",
        "\n",
        "The result, z, is the same shape as x and y.\n",
        "\n",
        "--------------------------------\n",
        "##ReLU Operation:\n",
        "z = np.maximum(z, 0.) replaces all negative values in z with 0, which is similar to applying the ReLU activation function.\n",
        "\n",
        "Since x and y are filled with random values between 0 and 1, the addition itself won't result in negative values, but this simulates how ReLU is typically applied.\n",
        "\n",
        "------------------------------------\n",
        "##Measuring the Total Time:\n",
        "\n",
        "    print(\"Took: {0:.2f} s\".format(time.time() - t0))\n",
        "\n",
        "After the loop finishes, the total time taken is calculated by subtracting the initial time t0 from the current time using time.time().\n",
        "\n",
        "The result is printed in seconds, formatted to 2 decimal places.\n",
        "\n",
        "This code demonstrates the efficiency of vectorized operations in NumPy:\n",
        "\n",
        "Addition of two arrays (x + y) and applying the ReLU-like operation (np.maximum(z, 0.)) are done in a vectorized manner, meaning the operations are performed on entire arrays at once without the need for explicit loops in Python.\n",
        "\n",
        "NumPy's internal C and Fortran optimizations make such vectorized operations much faster than if we were to use explicit Python loops.\n",
        "\n",
        "--------------------------------\n",
        "##Example Output:\n",
        "An example output might look like:\n",
        "\n",
        "    Took: 0.05 s\n",
        "\n",
        "This indicates that the operation took 0.05 seconds to complete 1,000 iterations.\n",
        "\n",
        "--------------------------------\n",
        "#Summary\n",
        "The code adds two random arrays (x and y) 1,000 times and applies a ReLU-like function to the result.\n",
        "\n",
        "It uses vectorized NumPy operations for efficient computation.\n",
        "The time taken to complete the operation is measured and printed.\n",
        "\n",
        "Would you like to optimize this further or add other operations to measure?"
      ],
      "metadata": {
        "id": "bW7E1ZZcjbBg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Broadcasting"
      ],
      "metadata": {
        "id": "nr0JquS1rEBR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "X = np.random.random((32, 10))\n",
        "y = np.random.random((10,))"
      ],
      "metadata": {
        "id": "ZJlj4vFfoqhF"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = np.expand_dims(y, axis=0)"
      ],
      "metadata": {
        "id": "zeNax11CrScJ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y = np.concatenate([y] * 32, axis=0)"
      ],
      "metadata": {
        "id": "QRVMW80jrYOG"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def naive_add_matrix_and_vector(x, y):\n",
        "    assert len(x.shape) == 2\n",
        "    assert len(y.shape) == 1\n",
        "    assert x.shape[1] == y.shape[0]\n",
        "    x = x.copy()\n",
        "    for i in range(x.shape[0]):\n",
        "        for j in range(x.shape[1]):\n",
        "            x[i, j] += y[j]\n",
        "    return x"
      ],
      "metadata": {
        "id": "MTQQb5K2rePt"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adding a vector to each row of a matrix\n",
        "The function naive_add_matrix_and_vector is a manual implementation of adding a vector to each row of a matrix.\n",
        "\n",
        "This is known as broadcasting in NumPy, and the function achieves it using nested loops.\n",
        "\n",
        "In the provided naive_add_matrix_and_vector function, the variables x and y are not related to the common usage of x (input features) and y (labels) in machine learning contexts.\n",
        "\n",
        "Instead, here:\n",
        "\n",
        "x is a matrix (a 2D array), which could represent any data structured in rows and columns (for example, features or data points).\n",
        "\n",
        "y is a vector (a 1D array), which could represent any set of values that need to be added to each row of the matrix x.\n",
        "\n",
        "##Key Points:\n",
        "The purpose of this function is to add the vector y to each row of the matrix x.\n",
        "\n",
        "This operation is commonly known as broadcasting in NumPy, and in neural networks, such an operation could resemble adding bias terms to every data point in a batch (though y is not strictly a bias here).\n",
        "\n",
        "There is no notion of \"labels\" involved in this function. The y in this context is simply a vector being added to the matrix.\n",
        "\n",
        "##Clarification of Usage:\n",
        "If x were features (e.g., pixel intensities of an image or sensor measurements), the vector y could represent a set of corrections, biases, or a transformation you want to apply to each feature across multiple data points (rows).\n",
        "\n",
        "In the context of neural networks or matrix operations, these names don't correspond to input (x) and target labels (y).\n",
        "\n",
        "They are just generic names for matrix and vector data being added element-wise.\n",
        "\n",
        "##In Terms of ReLU:\n",
        "If you were referring to the earlier use of ReLU, the operation here doesn’t directly involve the ReLU function.\n",
        "\n",
        "It is merely adding a vector y to each row of a matrix x. The ReLU function, on the other hand, sets negative values to zero.\n",
        "\n",
        "If the function were used in the context of neural networks, x might be an activation matrix from a layer of a neural network, and adding a vector y could represent adding bias terms to the activations before applying the ReLU operation.\n",
        "\n",
        "Would you like to clarify how this fits into the broader context you're working with, like neural networks or matrix manipulations?\n",
        "\n",
        "--------------------------\n",
        "## Assertions:\n",
        "\n",
        "    assert len(x.shape) == 2\n",
        "    assert len(y.shape) == 1\n",
        "    assert x.shape[1] == y.shape[0]\n",
        "\n",
        "\n",
        "These assertions check that:\n",
        "\n",
        "x is a 2D array (matrix).\n",
        "\n",
        "y is a 1D array (vector).\n",
        "\n",
        "The number of columns in x (x.shape[1]) matches the number of elements in y (y.shape[0]), ensuring that the vector can be added to each row of the matrix.\n",
        "\n",
        "--------------------------------\n",
        "##Copying the Input Matrix:\n",
        "\n",
        "    x = x.copy()\n",
        "\n",
        "This creates a copy of the matrix x so that the original matrix is not modified directly.\n",
        "\n",
        "##Nested Loops to Add Vector to Matrix:\n",
        "\n",
        "    for i in range(x.shape[0]):\n",
        "        for j in range(x.shape[1]):\n",
        "            x[i, j] += y[j]\n",
        "\n",
        "The outer loop for i in range(x.shape[0]) iterates over the rows of the matrix x (the number of rows is x.shape[0]).\n",
        "\n",
        "The inner loop for j in range(x.shape[1]) iterates over the columns of the matrix x (the number of columns is x.shape[1]).\n",
        "\n",
        "Inside the inner loop, the function adds the corresponding element from the vector y to the element in the matrix x.\n",
        "\n",
        "Specifically, for each row i and column j, the value x[i, j] is incremented by y[j].\n",
        "\n",
        "-------------------------------------\n",
        "#Return the Modified Matrix:\n",
        "\n",
        "    return x\n",
        "\n",
        "After applying the vector to each row of the matrix, the function returns the modified matrix x.\n",
        "\n",
        "--------------------\n",
        "#Example:\n",
        "Suppose x is a 2x3 matrix:\n",
        "\n",
        "    x = np.array([[1, 2, 3],\n",
        "                  [4, 5, 6]])\n",
        "\n",
        "And y is a 1D vector of length 3:\n",
        "\n",
        "    y = np.array([10, 20, 30])\n",
        "\n",
        "Calling naive_add_matrix_and_vector(x, y) will result in:\n",
        "\n",
        "First, the vector [10, 20, 30] is added to the first row: [1 + 10, 2 + 20, 3 + 30] = [11, 22, 33].\n",
        "\n",
        "Then, the vector is added to the second row: [4 + 10, 5 + 20, 6 + 30] = [14, 25, 36].\n",
        "\n",
        "The output will be:\n",
        "\n",
        "    array([[11, 22, 33],\n",
        "           [14, 25, 36]])\n",
        "\n",
        "------------------------------\n",
        "#Broadcasting in NumPy\n",
        "In NumPy, this operation is known as broadcasting, where NumPy automatically adds the vector to each row of the matrix without explicit loops.\n",
        "\n",
        "You can achieve the same operation more efficiently with NumPy like this:\n",
        "\n",
        "    z = x + y\n",
        "\n",
        "This automatically applies the vector y to each row of x, thanks to NumPy's broadcasting rules.\n",
        "\n",
        "--------------------------\n",
        "#Summary\n",
        "The function naive_add_matrix_and_vector manually adds a vector to each row of a matrix using nested loops.\n",
        "\n",
        "It checks for valid input shapes and then performs the addition element-wise.\n",
        "\n",
        "This operation can be more efficiently performed in NumPy using broadcasting (x + y)."
      ],
      "metadata": {
        "id": "FjK8ZgAyrlwf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "x = np.random.random((64, 3, 32, 10))\n",
        "y = np.random.random((32, 10))\n",
        "z = np.maximum(x, y)"
      ],
      "metadata": {
        "id": "kgTl5kZvvyP5"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Broadcasting Example 2\n",
        "This snippet demonstrates an example of broadcasting in NumPy, where operations between arrays of different shapes are performed without explicit looping.\n",
        "\n",
        "--------------------------\n",
        "Let’s break down each part of the code to understand how it works.\n",
        "\n",
        "##Creating Random Arrays:\n",
        "\n",
        "    x = np.random.random((64, 3, 32, 10))\n",
        "\n",
        "##x is a 4D array with shape (64, 3, 32, 10).\n",
        "\n",
        "This means that x contains 64 samples, each with 3 channels (or features), where each channel has a matrix of shape (32, 10).\n",
        "\n",
        "The values in x are random floats between 0 and 1.\n",
        "\n",
        "    y = np.random.random((32, 10))\n",
        "\n",
        "y is a 2D array with shape (32, 10).\n",
        "\n",
        "This is a matrix with 32 rows and 10 columns, filled with random floats between 0 and 1.\n",
        "\n",
        "------------------------\n",
        "#Applying the np.maximum Function:\n",
        "\n",
        "    z = np.maximum(x, y)\n",
        "\n",
        "np.maximum performs an element-wise maximum between the arrays x and y.\n",
        "\n",
        "For each element in x and y, it compares the values and returns the larger of the two.\n",
        "\n",
        "This works due to broadcasting in NumPy.\n",
        "\n",
        "----------------------\n",
        "#Broadcasting Explained:\n",
        "Broadcasting in NumPy allows arrays of different shapes to be used in arithmetic operations as long as the shapes are compatible.\n",
        "\n",
        "NumPy tries to expand smaller arrays along dimensions to match the larger array’s shape.\n",
        "\n",
        "---------------------------\n",
        "#Broadcasting Rules:\n",
        "Align the shapes of the arrays starting from the rightmost dimension.\n",
        "\n",
        "---------------------------\n",
        "#If dimensions are not equal:\n",
        "A dimension of size 1 can be stretched to match the larger dimension.\n",
        "\n",
        "Dimensions that are not 1 and don’t match will result in an error.\n",
        "\n",
        "--------------------------\n",
        "#Shape Alignment:\n",
        "    x has shape (64, 3, 32, 10).\n",
        "    y has shape (32, 10).\n",
        "\n",
        "To align these shapes:\n",
        "\n",
        "The rightmost two dimensions of x and y both have the shape (32, 10), so they are directly compatible for element-wise comparison.\n",
        "\n",
        "NumPy automatically broadcasts y across the first two dimensions of x.\n",
        "\n",
        "This means y is virtually \"expanded\" to match the first two dimensions of x, replicating the data in y for each sample and channel in x.\n",
        "\n",
        "In this case, y is repeated 64 times (for the first dimension) and 3 times (for the second dimension).\n",
        "\n",
        "--------------------------\n",
        "#Final Shape:\n",
        "The resulting array z will have the same shape as x, which is (64, 3, 32, 10), since the maximum operation is applied element-wise across the broadcasted arrays.\n",
        "\n",
        "--------------------\n",
        "#Example of the Operation:\n",
        "Suppose an element in x at position (0, 0, 0, 0) is 0.5, and the corresponding element in y at position (0, 0) is 0.7.\n",
        "\n",
        "The np.maximum function will compare these two values and choose the larger one, so in this case, z[0, 0, 0, 0] will be 0.7.\n",
        "\n",
        "##Key Points:\n",
        "x is a 4D array, and y is a 2D array.\n",
        "\n",
        "Due to broadcasting, y is replicated across the first two dimensions of x to match the shapes, and then an element-wise maximum is computed.\n",
        "\n",
        "The result z has the same shape as x.\n",
        "\n",
        "--------------------\n",
        "#Summary\n",
        "x is a 4D tensor, and y is a 2D tensor.\n",
        "\n",
        "Broadcasting allows NumPy to perform an element-wise maximum operation between x and y by aligning their shapes.\n",
        "\n",
        "The resulting tensor z has the shape (64, 3, 32, 10)."
      ],
      "metadata": {
        "id": "MK5YOpUCwTHp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tensor product"
      ],
      "metadata": {
        "id": "e3V_AUeN0qFn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.random.random((32,))\n",
        "y = np.random.random((32,))\n",
        "z = np.dot(x, y)"
      ],
      "metadata": {
        "id": "p84kHM5j0sgK"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def naive_vector_dot(x, y):\n",
        "    assert len(x.shape) == 1\n",
        "    assert len(y.shape) == 1\n",
        "    assert x.shape[0] == y.shape[0]\n",
        "    z = 0.\n",
        "    for i in range(x.shape[0]):\n",
        "        z += x[i] * y[i]\n",
        "    return z"
      ],
      "metadata": {
        "id": "uXu1dTND0yGq"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dot product\n",
        "The function naive_vector_dot implements a dot product (or inner product) between two vectors x and y manually, using a loop.\n",
        "\n",
        "This is a fundamental operation in linear algebra, particularly in machine learning, where it is often used in tasks such as computing the weighted sum of features in neural networks.\n",
        "\n",
        "----------------------\n",
        "Assertions:\n",
        "\n",
        "    assert len(x.shape) == 1\n",
        "    assert len(y.shape) == 1\n",
        "    assert x.shape[0] == y.shape[0]\n",
        "\n",
        "These checks ensure:\n",
        "\n",
        "x is a 1D vector (it has one dimension).\n",
        "\n",
        "y is a 1D vector.\n",
        "\n",
        "Both vectors have the same length (the number of elements in x and y must match for the dot product to be valid).\n",
        "\n",
        "------------------------------\n",
        "Initialize the Dot Product:\n",
        "\n",
        "    z = 0\n",
        "\n",
        "The result of the dot product will be stored in z, which starts at 0.\n",
        "\n",
        "-------------------------\n",
        "#Loop Through the Elements of x and y:\n",
        "\n",
        "    for i in range(x.shape[0]):\n",
        "       z += x[i] * y[i]\n",
        "\n",
        "The loop iterates over each element in the vectors. x.shape[0] gives the length of the vectors.\n",
        "\n",
        "For each element i, it multiplies the corresponding elements from x and y (x[i] * y[i]), and adds the result to z.\n",
        "\n",
        "-------------------------------\n",
        "#Return the Dot Product:\n",
        "    return z\n",
        "\n",
        "After the loop finishes, the function returns z, which now holds the sum of all element-wise products of x and y.\n",
        "\n",
        "-------------------------\n",
        "#Example\n",
        "Suppose we have two vectors:\n",
        "\n",
        "    x = np.array([1, 2, 3])\n",
        "    y = np.array([4, 5, 6])\n",
        "\n",
        "The dot product is calculated as:\n",
        "\n",
        "    1*4 + 2*5 + 3*6 = 4 + 10 + 18 = 32\n",
        "\n",
        "So, naive_vector_dot(x, y) would return 32.\n",
        "\n",
        "-----------------------------\n",
        "#Explanation of the Dot Product:\n",
        "The dot product of two vectors is the sum of the products of their corresponding elements. Mathematically, for two vectors    \n",
        "    x = [x1, x2, ..., xn] and y = [y1, y2, ..., yn]\n",
        "    \n",
        "the dot product is:\n",
        "\n",
        "    x · y = x1*y1 + x2*y2 + ... + xn*yn\n",
        "\n",
        "---------------------------\n",
        "##In NumPy\n",
        "\n",
        "You could achieve the same operation more efficiently in NumPy using:\n",
        "\n",
        "    z = np.dot(x, y)\n",
        "\n",
        "This function does the same thing but is much faster because it's optimized for performance.\n",
        "\n",
        "-------------------------------\n",
        "#Summary\n",
        "This function calculates the dot product between two 1D vectors manually, using a loop.\n",
        "\n",
        "It checks that the vectors are of the same length and returns the sum of the element-wise products.\n",
        "\n",
        "The dot product is a key operation in many machine learning algorithms, such as computing the weighted sum of features or activations in neural networks."
      ],
      "metadata": {
        "id": "8Y59KaNy1zDP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def naive_matrix_vector_dot(x, y):\n",
        "    assert len(x.shape) == 2\n",
        "    assert len(y.shape) == 1\n",
        "    assert x.shape[1] == y.shape[0]\n",
        "    z = np.zeros(x.shape[0])\n",
        "    for i in range(x.shape[0]):\n",
        "        for j in range(x.shape[1]):\n",
        "            z[i] += x[i, j] * y[j]\n",
        "    return z"
      ],
      "metadata": {
        "id": "LEwNcVVsIsjk"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Naive_matrix_vector_dot multiplication\n",
        "The function naive_matrix_vector_dot performs the matrix-vector multiplication between a matrix x and a vector y.\n",
        "\n",
        "This operation is fundamental in linear algebra and is widely used in machine learning tasks like computing the activations in a neural network layer.\n",
        "\n",
        "-------------------------------\n",
        "##Assertions:\n",
        "\n",
        "    assert len(x.shape) == 2\n",
        "    assert len(y.shape) == 1\n",
        "    assert x.shape[1] == y.shape[0]\n",
        "\n",
        "These checks ensure:\n",
        "\n",
        "x is a 2D matrix (it has two dimensions, i.e., rows and columns).\n",
        "\n",
        "y is a 1D vector.\n",
        "\n",
        "The number of columns in the matrix x (i.e., x.shape[1]) must match the number of elements in the vector y (i.e., y.shape[0]).\n",
        "\n",
        "This is necessary for matrix-vector multiplication to be valid.\n",
        "\n",
        "-----------------------------\n",
        "#Initialize the Result Vector:\n",
        "\n",
        "    z = np.zeros(x.shape[0])\n",
        "\n",
        "z is initialized as a vector of zeros with the same length as the number of rows in x (i.e., x.shape[0]).\n",
        "\n",
        "This will hold the result of the matrix-vector multiplication.\n",
        "Loop Through the Rows and Columns:\n",
        "\n",
        "    for i in range(x.shape[0]):\n",
        "       for j in range(x.shape[1]):\n",
        "          z[i] += x[i, j] * y[j]\n",
        "\n",
        "The outer loop iterates over the rows of x (i runs over x.shape[0]).\n",
        "\n",
        "The inner loop iterates over the columns of x (j runs over x.shape[1]).\n",
        "\n",
        "For each element in row i of matrix x, it multiplies x[i, j] (the element at row i, column j) with y[j] (the corresponding element in vector y) and adds the result to z[i].\n",
        "\n",
        "Essentially, this is calculating the dot product between row i of matrix x and vector y.\n",
        "\n",
        "--------------------------\n",
        "#Return the Result:\n",
        "\n",
        "    return z\n",
        "\n",
        "After all the rows and columns have been processed, the function returns the result vector z, which contains the matrix-vector product.\n",
        "\n",
        "-----------------------\n",
        "#Example\n",
        "Let's consider an example with a matrix x and a vector y:\n",
        "\n",
        "    x = np.array([[1, 2, 3], [4, 5, 6]])  # Shape: (2, 3)\n",
        "    y = np.array([1, 2, 3])               # Shape: (3,)\n",
        "\n",
        "Matrix x has 2 rows and 3 columns, and vector y has 3 elements.\n",
        "\n",
        "The function computes the dot product between each row of x and y:\n",
        "\n",
        "For the first row of x ([1, 2, 3]) and y ([1, 2, 3]):\n",
        "\n",
        "    1*1 + 2*2 + 3*3 = 1 + 4 + 9 = 14\n",
        "\n",
        "For the second row of x ([4, 5, 6]) and y ([1, 2, 3]):\n",
        "\n",
        "    4*1 + 5*2 + 6*3 = 4 + 10 + 18 = 32\n",
        "\n",
        "\n",
        "##So, the result vector z will be:\n",
        "\n",
        "    z = np.array([14, 32])  # Shape: (2,)\n",
        "\n",
        "------------------------------\n",
        "#Matrix-Vector Multiplication:\n",
        "Matrix-vector multiplication is a key operation where, given a matrix x of shape (m, n) and a vector y of shape (n,), the result is a vector z of shape (m,).\n",
        "\n",
        "Each element of z is the dot product of the corresponding row of x with the vector y.\n",
        "\n",
        "#In NumPy:\n",
        "You could achieve the same operation more efficiently using:\n",
        "\n",
        "    z = np.dot(x, y)\n",
        "\n",
        "This performs matrix-vector multiplication in a highly optimized manner.\n",
        "\n",
        "-----------------------\n",
        "#Summary\n",
        "The function naive_matrix_vector_dot multiplies a matrix x (2D) by a vector y (1D) manually, using nested loops.\n",
        "\n",
        "It computes the dot product of each row in the matrix with the vector and stores the result in a new vector z.\n",
        "\n",
        "Matrix-vector multiplication is crucial in various linear algebra and machine learning tasks."
      ],
      "metadata": {
        "id": "5245mKACJF0Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def naive_matrix_vector_dot(x, y):\n",
        "    z = np.zeros(x.shape[0])\n",
        "    for i in range(x.shape[0]):\n",
        "        z[i] = naive_vector_dot(x[i, :], y)\n",
        "    return z"
      ],
      "metadata": {
        "id": "d1XB_Z12N4Wt"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def naive_matrix_dot(x, y):\n",
        "    assert len(x.shape) == 2\n",
        "    assert len(y.shape) == 2\n",
        "    assert x.shape[1] == y.shape[0]\n",
        "    z = np.zeros((x.shape[0], y.shape[1]))\n",
        "    for i in range(x.shape[0]):\n",
        "        for j in range(y.shape[1]):\n",
        "            row_x = x[i, :]\n",
        "            column_y = y[:, j]\n",
        "            z[i, j] = naive_vector_dot(row_x, column_y)\n",
        "    return z"
      ],
      "metadata": {
        "id": "1AoBOgkNN71s"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tensor reshaping"
      ],
      "metadata": {
        "id": "dZsOC9-lOfJa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PL0yQ9ZpQGVr",
        "outputId": "746255b2-6a3e-467b-eb3e-b68b29c868d0"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_images = train_images.reshape((60000, 28 * 28))"
      ],
      "metadata": {
        "id": "OYFhusW8OiuT"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x=np.array([[0., 1.],\n",
        "            [2., 3.],\n",
        "            [4., 5.0]])\n",
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OUESfH-nQhzp",
        "outputId": "ae172763-d497-4130-c095-b0b17e090299"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = x.reshape(6, 1)\n",
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g3oOeX2MRz_L",
        "outputId": "3421b727-87a0-4e1e-8a00-aa80078faae9"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.],\n",
              "       [1.],\n",
              "       [2.],\n",
              "       [3.],\n",
              "       [4.],\n",
              "       [5.]])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Matrices (rank-2 tensors)"
      ],
      "metadata": {
        "id": "IlALRVYeCuu2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.array([[5, 78, 2, 34, 0],\n",
        "              [6, 79, 3, 35, 1],\n",
        "              [7, 80, 4, 36, 2]])\n",
        "x.ndim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4vcifHBkC0KT",
        "outputId": "424d9e6f-1239-4112-9cec-173c1795169a"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x= np.array([[5, 78, 2, 34, 0],\n",
        "            [6, 79, 3, 35, 1],\n",
        "            [7, 80, 4, 36, 2]])\n",
        "x.ndim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-TdOguoxDUFJ",
        "outputId": "74d7feb8-13f0-4582-9805-1dd05de3b251"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Rank-3 and higher-rank tensors"
      ],
      "metadata": {
        "id": "8chkkG9JC6Pv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.array([[[5, 78, 2, 34, 0],\n",
        "               [6, 79, 3, 35, 1],\n",
        "               [7, 80, 4, 36, 2]],\n",
        "              [[5, 78, 2, 34, 0],\n",
        "               [6, 79, 3, 35, 1],\n",
        "               [7, 80, 4, 36, 2]],\n",
        "              [[5, 78, 2, 34, 0],\n",
        "               [6, 79, 3, 35, 1],\n",
        "               [7, 80, 4, 36, 2]]])\n",
        "x.ndim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AYRJpdpwHEOD",
        "outputId": "204a110b-eaad-437c-d545-4e14ed04e911"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Key attributes"
      ],
      "metadata": {
        "id": "SZ8D_zMgHi0H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
      ],
      "metadata": {
        "id": "QRGjcdkCHm0T"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_images.ndim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oszzi7uUJMpP",
        "outputId": "2326387e-92c3-40b4-8e76-d69a03bef8f1"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels.ndim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7k1TF84JTA1",
        "outputId": "6393726d-a5c3-4767-800f-a9519126ef23"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_images.dtype"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AtwhHFL0LPu5",
        "outputId": "1f98624f-d007-4c4f-d243-84690e6441e0"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dtype('uint8')"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Displaying the fourth digit"
      ],
      "metadata": {
        "id": "dFW9shNPNwRB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "digit = train_images[4]\n",
        "plt.imshow(digit, cmap=plt.cm.binary)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R4Ws2BWvNzq2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "outputId": "bbf8b9a5-544f-409c-fe30-a80eb2059aa8"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Changing 'train_image' to 'train_images' assuming this is the intended variable\n",
        "digit = train_images[4]\n",
        "plt.imshow(digit, cmap=plt.cm.binary)\n",
        "plt.show()"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "jzNNN4xgOt5A",
        "outputId": "ea017e3f-2ec5-447b-800a-4425a4a2d343"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Slicing"
      ],
      "metadata": {
        "id": "FtTLp-E0dhJk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_slice = train_images[10:100, :, :]\n",
        "my_slice.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZbkJr9fmdgbn",
        "outputId": "44e9b8e4-a2b3-407f-82f3-bbee9899763b"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(90, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this line of code:\n",
        "\n",
        "    my_slice = train_images[10:100, :, :]\n",
        "\n",
        "you are slicing the train_images array and retrieving a subset of the data. Here's a detailed breakdown:\n",
        "\n",
        "#train_images[10:100]:\n",
        "\n",
        "This selects rows (or images) from index 10 to index 99 (since Python slicing is exclusive of the end index).\n",
        "\n",
        "#:\n",
        "This selects all pixels along the second dimension (height) of the images.\n",
        "\n",
        "#:\n",
        "This selects all pixels along the third dimension (width) of the images.\n",
        "\n",
        "The result, my_slice, will contain the images from index 10 to 99, and the shape of the sliced tensor is determined by the number of images and their height and width.\n",
        "\n",
        "For example, if train_images has a shape of (60000, 28, 28)—meaning 60,000 images of 28x28 pixels—then:\n",
        "\n",
        "train_images[10:100, :, :] will select 90 images (from index 10 to 99).\n",
        "\n",
        "The shape of my_slice will be (90, 28, 28).\n",
        "\n",
        "So, when you run my_slice.shape, it will output:\n",
        "\n",
        "    (90, 28, 28)\n",
        "    \n",
        "This indicates that you now have a subset of 90 images, each of size 28x28 pixels."
      ],
      "metadata": {
        "id": "AWeSffW6g4C-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The notion of data batches"
      ],
      "metadata": {
        "id": "AWmKbDv2h9R3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch = train_images[:128]"
      ],
      "metadata": {
        "id": "zq7q1VbEiBva"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the following code:\n",
        "\n",
        "    batch = train_images[:128]\n",
        "\n",
        "you are selecting a batch of images from the train_images dataset. Here’s what is happening:\n",
        "\n",
        "#train_images[:128]\n",
        "selects the first 128 images from the train_images array.\n",
        "\n",
        "The slice :128 takes images from index 0 to 127 (since slicing is exclusive of the end index).\n",
        "\n",
        "This is typically done when working with mini-batches in machine learning, where instead of processing all the images at once, you process a smaller batch at a time (for memory and computational efficiency).\n",
        "\n",
        "If, for example, train_images has the shape (60000, 28, 28), representing 60,000 images of 28x28 pixels, then:\n",
        "\n",
        "train_images[:128] will select the first 128 images, giving batch a shape of (128, 28, 28).\n",
        "\n",
        "-----------------\n",
        "#In summary:\n",
        "\n",
        "batch now contains 128 images.\n",
        "\n",
        "Each image is 28x28 pixels if you are using the MNIST dataset (or similar datasets).\n",
        "\n",
        "This subset can be used for training or evaluation in small, manageable chunks."
      ],
      "metadata": {
        "id": "zEdGLx3Oifk2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch = train_images[128:256]"
      ],
      "metadata": {
        "id": "0IN27xWJjDQt"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n = 3\n",
        "batch = train_images[128 * n:128 * (n + 1)]"
      ],
      "metadata": {
        "id": "eupggP6RjPXF"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this code snippet:\n",
        "    n = 3\n",
        "    batch = train_images[128 * n:128 * (n + 1)]\n",
        "\n",
        "you are selecting a batch of images from the train_images dataset, starting from the index 128 * n and going up to 128 * (n + 1) (not inclusive of 128 * (n + 1)).\n",
        "\n",
        "----------------------\n",
        "Here’s the breakdown:\n",
        "\n",
        "#n = 3:\n",
        "\n",
        "This sets n to 3, which will be used to calculate the starting and ending indices for slicing.\n",
        "\n",
        "#128 * n and 128 * (n + 1):\n",
        "\n",
        "These are the start and end indices for slicing the batch.\n",
        "\n",
        "#128 * 3 = 384:\n",
        "\n",
        "The start index for this batch.\n",
        "\n",
        "#128 * (3 + 1) = 512:\n",
        "\n",
        "The end index for this batch (not inclusive).\n",
        "\n",
        "So, this code is selecting the images from index 384 to 511, i.e., the fourth batch of 128 images (since n = 3 means you’re selecting the batch starting from the 384th image).\n",
        "\n",
        "If train_images has a shape of (60000, 28, 28) (for example), then:\n",
        "\n",
        "batch will contain 128 images, each of size 28x28 pixels.\n",
        "\n",
        "The shape of batch will be (128, 28, 28).\n",
        "\n",
        "\n",
        "---------------------------\n",
        "#Generalized Concept:\n",
        "The value n indicates which batch you are selecting. For example:\n",
        "\n",
        "If n = 0, you get the first batch (images from 0 to 127).\n",
        "\n",
        "If n = 1, you get the second batch (images from 128 to 255), and so on.\n",
        "\n",
        "This kind of batching is commonly used in training neural networks, where you process one batch of data at a time."
      ],
      "metadata": {
        "id": "CUA-CxAEjb0K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Looking back at our first example"
      ],
      "metadata": {
        "id": "YZK_ci__kzOB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "train_images = train_images.reshape((60000, 28 * 28))\n",
        "train_images = train_images.astype(\"float32\") / 255\n",
        "test_images = test_images.reshape((10000, 28 * 28))\n",
        "test_images = test_images.astype(\"float32\") / 255"
      ],
      "metadata": {
        "id": "by8KL6y9k2yK"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "source": [
        "!pip install tensorflow\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "model = keras.Sequential([\n",
        "    layers.Dense(512, activation=\"relu\"),\n",
        "    layers.Dense(10, activation=\"softmax\")\n",
        "])"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_yQShsdglTPH",
        "outputId": "e1daef14-2fe0-487b-de30-c6c11fcc7936"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.0)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.9.2)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.13.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (3.0.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "d9Z0iIEMlhQg"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Optimizer: rmsprop\n",
        "    optimizer=\"rmsprop\"\n",
        "\n",
        "You are using the RMSprop (Root Mean Square Propagation) optimizer.\n",
        "\n",
        "This is a commonly used optimization algorithm designed to adapt the learning rate dynamically for each parameter during training.\n",
        "\n",
        "It works particularly well in cases where the gradients can fluctuate significantly.\n",
        "\n",
        "RMSprop divides the gradient by a running average of its recent magnitudes, making it well-suited for training models on noisy data or models that require adaptive learning rates.\n",
        "\n",
        "----------------------\n",
        "#2. Loss: sparse_categorical_crossentropy\n",
        "    loss=\"sparse_categorical_crossentropy\"\n",
        "\n",
        "This loss function is used when you have a classification problem where your labels are integers (as opposed to one-hot encoded vectors).\n",
        "\n",
        "Sparse categorical crossentropy is useful for multi-class classification tasks where each sample belongs to one of multiple classes.\n",
        "\n",
        "It expects the labels to be in integer format, and it works by comparing the predicted probability distribution over classes to the true label (integer class index).\n",
        "\n",
        "Example: For digit classification (such as MNIST), where the labels are integers representing digits (0, 1, 2, ..., 9), you can use this loss function.\n",
        "\n",
        "-----------------------------\n",
        "#3. Metrics: accuracy\n",
        "    metrics=[\"accuracy\"]\n",
        "\n",
        "You are using accuracy as the metric to evaluate your model's performance during training and validation.\n",
        "\n",
        "Accuracy measures the percentage of predictions that match the true labels.\n",
        "\n",
        "It is a common evaluation metric for classification tasks where you're predicting categorical values (e.g., classifying images of digits).\n",
        "\n",
        "--------------------------\n",
        "#Summary\n",
        "You are using the RMSprop optimizer, which adapts the learning rate during training.\n",
        "\n",
        "The model's loss is computed using sparse categorical crossentropy, which is appropriate for multi-class classification problems with integer labels.\n",
        "\n",
        "You track accuracy during training to monitor how often the model's predictions match the true labels.\n",
        "\n",
        "This setup is typically used for tasks like image classification with datasets such as MNIST or CIFAR-10."
      ],
      "metadata": {
        "id": "7cAR_IJCmWfr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(train_images, train_labels, epochs=5, batch_size=128)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HbMjyNjnlrkv",
        "outputId": "6e96747f-3398-4c9c-cb82-3bacb40568c8"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 22ms/step - accuracy: 0.8771 - loss: 0.4328\n",
            "Epoch 2/5\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 15ms/step - accuracy: 0.9665 - loss: 0.1131\n",
            "Epoch 3/5\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 12ms/step - accuracy: 0.9786 - loss: 0.0742\n",
            "Epoch 4/5\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - accuracy: 0.9851 - loss: 0.0507\n",
            "Epoch 5/5\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - accuracy: 0.9893 - loss: 0.0380\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7dcce090b610>"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "wlFOkmsXnSRj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A simple Dense class"
      ],
      "metadata": {
        "id": "s6P3xlUDlqah"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "class NaiveDense:\n",
        "    def __init__(self, input_size, output_size, activation):\n",
        "        self.activation = activation\n",
        "\n",
        "        w_shape = (input_size, output_size)\n",
        "        w_initial_value = tf.random.uniform(w_shape, minval=0, maxval=1e-1)\n",
        "        self.W = tf.Variable(w_initial_value)\n",
        "\n",
        "        b_shape = (output_size,)\n",
        "        b_initial_value = tf.zeros(b_shape)\n",
        "        self.b = tf.Variable(b_initial_value)\n",
        "\n",
        "    def __call__(self, inputs):\n",
        "        return self.activation(tf.matmul(inputs, self.W) + self.b)\n",
        "\n",
        "    @property\n",
        "    def weights(self):\n",
        "        return [self.W, self.b]"
      ],
      "metadata": {
        "id": "lNDLrIKznh99"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradient Descent Velocity & momentum"
      ],
      "metadata": {
        "id": "sx76i2GDuMuk"
      }
    },
    {
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# ... (other code remains the same)\n",
        "\n",
        "# Recreate your model with correct input_shape\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Dense(1, input_shape=(2,))  # Adjusted to match input_data shape\n",
        "])\n",
        "\n",
        "# Initialize loss to a higher value to ensure the loop is entered\n",
        "loss = 10.0  # Increased initial loss value\n",
        "past_velocity = 0.\n",
        "momentum = 0.1\n",
        "learning_rate = 0.01\n",
        "max_iterations = 1000\n",
        "iteration = 0\n",
        "\n",
        "while loss > 0.01 and iteration < max_iterations:\n",
        "    w, loss, gradient = get_current_parameters(inputs_data, targets_data, loss_fn)\n",
        "    velocity = past_velocity * momentum - learning_rate * gradient\n",
        "    w = w + momentum * velocity - learning_rate * gradient\n",
        "    past_velocity = velocity\n",
        "    update_parameter(w)\n",
        "\n",
        "    iteration += 1\n",
        "\n",
        "    # Print loss every iteration for monitoring\n",
        "    print(f\"Iteration: {iteration}, Loss: {loss.numpy()}\")  # Print loss here\n",
        "\n",
        "if iteration == max_iterations:\n",
        "    print(\"Maximum iterations reached without converging to desired loss.\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hvWSeMIt4xk_",
        "outputId": "978891ae-c011-4161-f420-34a7823fce83"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 1, Loss: 0.9156920313835144\n",
            "Iteration: 2, Loss: 0.7253199219703674\n",
            "Iteration: 3, Loss: 0.5730760097503662\n",
            "Iteration: 4, Loss: 0.4526579976081848\n",
            "Iteration: 5, Loss: 0.3575310707092285\n",
            "Iteration: 6, Loss: 0.28239402174949646\n",
            "Iteration: 7, Loss: 0.22304752469062805\n",
            "Iteration: 8, Loss: 0.17617282271385193\n",
            "Iteration: 9, Loss: 0.13914920389652252\n",
            "Iteration: 10, Loss: 0.10990619659423828\n",
            "Iteration: 11, Loss: 0.08680897206068039\n",
            "Iteration: 12, Loss: 0.0685654878616333\n",
            "Iteration: 13, Loss: 0.0541561096906662\n",
            "Iteration: 14, Loss: 0.04277491196990013\n",
            "Iteration: 15, Loss: 0.03378549963235855\n",
            "Iteration: 16, Loss: 0.026685204356908798\n",
            "Iteration: 17, Loss: 0.02107722871005535\n",
            "Iteration: 18, Loss: 0.01664775423705578\n",
            "Iteration: 19, Loss: 0.013149162754416466\n",
            "Iteration: 20, Loss: 0.010385768488049507\n",
            "Iteration: 21, Loss: 0.008203135803341866\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Initial Setup:\n",
        "    past_velocity = 0.\n",
        "    momentum = 0.1\n",
        "##past_velocity = 0.:\n",
        "This initializes the velocity to zero at the start of the optimization.\n",
        "\n",
        "##momentum = 0.1:\n",
        "The momentum term is set to 0.1.\n",
        "\n",
        "The momentum helps smooth out the update steps by adding a fraction of the previous velocity to the current one.\n",
        "\n",
        "It helps speed up convergence and dampen oscillations in the gradient descent process.\n",
        "\n",
        "----------------------------------\n",
        "# 2. Main Loop (while loss > 0.01):\n",
        "This loop runs until the loss becomes less than or equal to 0.\n",
        "\n",
        "##01\n",
        "meaning the optimization continues until the model's loss is sufficiently minimized.\n",
        "\n",
        "---------------------------------------\n",
        "\n",
        "#3. Retrieve Current Parameters:\n",
        "\n",
        "    w, loss, gradient = get_current_parameters()\n",
        "##w:\n",
        "The current weights (parameters) of the model.\n",
        "\n",
        "##loss:\n",
        "The current loss value of the model.\n",
        "\n",
        "##gradient:\n",
        "The gradient of the loss with respect to the weights, which tells us how the loss changes as we adjust the weights.\n",
        "\n",
        "-----------------------------\n",
        "#4. Velocity Update:\n",
        "\n",
        "    velocity = past_velocity * momentum - learning_rate * gradient\n",
        "This is where the momentum concept comes into play.\n",
        "\n",
        "The new velocity is computed as a combination of:\n",
        "\n",
        "##The previous velocity (past_velocity * momentum):\n",
        "\n",
        "This term incorporates the previous step’s velocity to maintain a smooth and faster path towards the minimum.\n",
        "\n",
        "##The current gradient update (- learning_rate * gradient):\n",
        "This is the usual gradient update, where you subtract the product of the learning rate and gradient from the current parameter value.\n",
        "\n",
        "By adding momentum, we prevent the optimizer from oscillating and help it move more steadily toward the minimum.\n",
        "\n",
        "------------------------------------\n",
        "#5. Update Parameters:\n",
        "    w = w + momentum * velocity - learning_rate * gradient\n",
        "\n",
        "The weights w are updated by applying the momentum and gradient terms:\n",
        "\n",
        "##momentum * velocity:\n",
        "\n",
        "This accelerates the gradient descent based on past velocities, helping to smooth the trajectory toward the minimum.\n",
        "\n",
        "##- learning_rate * gradient:\n",
        "This is the standard gradient descent step, where the parameters move in the direction of the negative gradient to reduce the loss.\n",
        "\n",
        "------------------------------------\n",
        "#6. Save the Current Velocity:\n",
        "\n",
        "    past_velocity = velocity\n",
        "\n",
        "After updating w, the current velocity is saved as past_velocity for use in the next iteration.\n",
        "\n",
        "---------------------------------------\n",
        "#7. Update Weights:\n",
        "\n",
        "    update_parameter(w)\n",
        "\n",
        "The new parameter values w are saved back into the model for the next iteration.\n",
        "\n",
        "----------------------------------\n",
        "#Summary of Momentum-Based Gradient Descent:\n",
        "Momentum is used to accelerate gradient descent in the relevant direction and dampen oscillations.\n",
        "\n",
        "#The key idea is to combine the current gradient update with a fraction of the previous velocity to create smoother, faster convergence.\n",
        "\n",
        "The velocity accumulates previous gradients, allowing the optimizer to \"remember\" past steps and avoid oscillations, especially in regions of the loss landscape with high curvature.\n",
        "\n",
        "This technique helps gradient descent converge faster in complex optimization problems with large datasets or when the gradients vary significantly over time."
      ],
      "metadata": {
        "id": "9XCCz1rhelam"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Initial Setup:\n",
        "past_velocity = 0.\n",
        "momentum = 0.1\n",
        "past_velocity = 0.:\n",
        "This initializes the velocity to zero at the start of the optimization.\n",
        "\n",
        "momentum = 0.1:\n",
        "The momentum term is set to 0.1.\n",
        "\n",
        "The momentum helps smooth out the update steps by adding a fraction of the previous velocity to the current one.\n",
        "\n",
        "It helps speed up convergence and dampen oscillations in the gradient descent process.\n",
        "\n",
        "2. Main Loop (while loss > 0.01):\n",
        "This loop runs until the loss becomes less than or equal to 0.\n",
        "\n",
        "01\n",
        "meaning the optimization continues until the model's loss is sufficiently minimized.\n",
        "\n",
        "3. Retrieve Current Parameters:\n",
        "w, loss, gradient = get_current_parameters()\n",
        "w:\n",
        "The current weights (parameters) of the model.\n",
        "\n",
        "loss:\n",
        "The current loss value of the model.\n",
        "\n",
        "gradient:\n",
        "The gradient of the loss with respect to the weights, which tells us how the loss changes as we adjust the weights.\n",
        "\n",
        "4. Velocity Update:\n",
        "velocity = past_velocity * momentum - learning_rate * gradient\n",
        "This is where the momentum concept comes into play.\n",
        "\n",
        "The new velocity is computed as a combination of:\n",
        "\n",
        "The previous velocity (past_velocity * momentum):\n",
        "This term incorporates the previous step’s velocity to maintain a smooth and faster path towards the minimum.\n",
        "\n",
        "The current gradient update (- learning_rate * gradient):\n",
        "This is the usual gradient update, where you subtract the product of the learning rate and gradient from the current parameter value.\n",
        "\n",
        "By adding momentum, we prevent the optimizer from oscillating and help it move more steadily toward the minimum.\n",
        "\n",
        "5. Update Parameters:\n",
        "w = w + momentum * velocity - learning_rate * gradient\n",
        "The weights w are updated by applying the momentum and gradient terms:\n",
        "\n",
        "momentum * velocity:\n",
        "This accelerates the gradient descent based on past velocities, helping to smooth the trajectory toward the minimum.\n",
        "\n",
        "- learning_rate * gradient:\n",
        "This is the standard gradient descent step, where the parameters move in the direction of the negative gradient to reduce the loss.\n",
        "\n",
        "6. Save the Current Velocity:\n",
        "past_velocity = velocity\n",
        "After updating w, the current velocity is saved as past_velocity for use in the next iteration.\n",
        "\n",
        "7. Update Weights:\n",
        "update_parameter(w)\n",
        "The new parameter values w are saved back into the model for the next iteration.\n",
        "\n",
        "Summary of Momentum-Based Gradient Descent:\n",
        "Momentum is used to accelerate gradient descent in the relevant direction and dampen oscillations.\n",
        "\n",
        "The key idea is to combine the current gradient update with a fraction of the previous velocity to create smoother, faster convergence.\n",
        "The velocity accumulates previous gradients, allowing the optimizer to \"remember\" past steps and avoid oscillations, especially in regions of the loss landscape with high curvature.\n",
        "\n",
        "This technique helps gradient descent converge faster in complex optimization problems with large datasets or when the gradients vary significantly over time.\n",
        "\n"
      ],
      "metadata": {
        "id": "kRRGYCFOuiqf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The engine of neural networks: gradient-based optimization"
      ],
      "metadata": {
        "id": "hUQkCbR-qdSj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "x = tf.Variable(0.)\n",
        "with tf.GradientTape() as tape:\n",
        "    y = 2 * x + 3\n",
        "grad_of_y_wrt_x = tape.gradient(y, x)"
      ],
      "metadata": {
        "id": "cqk8i7cGqc1X"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Automatic differentiation mechanism using tf.GradientTape\n",
        "\n",
        "Here’s an explanation of what’s happening step-by-step:\n",
        "\n",
        "-----------------------\n",
        "# 1. x = tf.Variable(0.)\n",
        "You create a TensorFlow variable x initialized to 0.0.\n",
        "\n",
        "The tf.Variable is a special type of tensor that allows for mutation, meaning its value can be updated during training or other operations.\n",
        "\n",
        "--------------------------------\n",
        "#2. with tf.GradientTape() as tape:\n",
        "The tf.GradientTape context is used to record operations for automatic differentiation.\n",
        "\n",
        "Inside this context, TensorFlow keeps track of any operation that involves trainable variables like x.\n",
        "\n",
        "These recorded operations allow TensorFlow to compute the gradient of the output with respect to any trainable variable inside the context.\n",
        "\n",
        "----------------------------\n",
        "#3. y = 2 * x + 3\n",
        "This defines the operation you're performing on x. Here, y is a function of x:\n",
        "\n",
        "$$y=2x+3$$\n",
        "\n",
        "This is a simple linear equation.\n",
        "\n",
        "--------------------------\n",
        "#4. grad_of_y_wrt_x = tape.gradient(y, x)\n",
        "After the context block (with), TensorFlow computes the gradient of y with respect to x using tape.gradient.\n",
        "\n",
        "The derivative (or gradient) of\n",
        "\n",
        "$$y=2x+3$$\n",
        "\n",
        "with respect to 𝑥, is simply 2 because:\n",
        "\n",
        "$$𝑑\n",
        "𝑦/\n",
        "𝑑\n",
        "𝑥\n",
        "=\n",
        "2\n",
        "dx/\n",
        "dy\n",
        "​=2$$\n",
        "\n",
        "So, the value of grad_of_y_wrt_x will be 2.0.\n",
        "\n",
        "Full Explanation of Gradient Computation:\n",
        "\n",
        "--------------------------\n",
        "#Automatic Differentiation:\n",
        "\n",
        "TensorFlow's tf.GradientTape is used for automatic differentiation, allowing you to compute gradients of scalar functions with respect to their inputs (variables like x).\n",
        "\n",
        "Gradient of y with respect to x: In this case, the gradient of\n",
        "\n",
        "$$y=2x+3$$\n",
        "\n",
        "with respect to 𝑥 is constant, equal to 2, because the slope of this linear function is always 2.\n",
        "\n",
        "-----------------------\n",
        "#Output\n",
        "\n",
        "print(grad_of_y_wrt_x)  # Will print 2.0\n",
        "\n",
        "This code would output 2.0 as the gradient of 𝑦 with respect to 𝑥."
      ],
      "metadata": {
        "id": "ElESa-uQqv1C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = tf.Variable(tf.random.uniform((2, 2)))\n",
        "with tf.GradientTape() as tape:\n",
        "    y = 2 * x + 3\n",
        "grad_of_y_wrt_x = tape.gradient(y, x)"
      ],
      "metadata": {
        "id": "s83OmR6g3ncM"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Performing automatic differentiation in TensorFlow on a 2x2 matrix (tensor) using tf.GradientTape.\n",
        "\n",
        "Let's break down what’s happening step by step:\n",
        "\n",
        "---------------------------\n",
        "# 1. x = tf.Variable(tf.random.uniform((2, 2)))\n",
        "You create a 2x2 matrix x as a TensorFlow variable.\n",
        "\n",
        "The matrix is filled with random values from a uniform distribution between 0 and 1.\n",
        "\n",
        "Since you use tf.Variable, this tensor is trainable, and TensorFlow can track operations performed on it for gradient computation.\n",
        "\n",
        "--------------------------------------\n",
        "# 2. with tf.GradientTape() as tape:\n",
        "This block records operations on x so that you can later compute the gradient of y with respect to x.\n",
        "\n",
        "---------------------------------------\n",
        "#3. y = 2 * x + 3\n",
        "This operation is performed element-wise on x. Each element of x is multiplied by 2, and then 3 is added to it.\n",
        "\n",
        "Since x is a 2x2 matrix, y will also be a 2x2 matrix, where each element follows the equation:\n",
        "\n",
        "$$y=2x+3$$\n",
        "\n",
        "--------------------------------\n",
        "#4. grad_of_y_wrt_x = tape.gradient(y, x)\n",
        "Here, you're computing the gradient of y with respect to x. Since\n",
        "\n",
        "$$y=2x+3$$\n",
        "\n",
        "is a linear function of 𝑥, the derivative (gradient) of\n",
        "𝑦 with respect to 𝑥, x is just 2 for every element of x.\n",
        "\n",
        "This is because:\n",
        "\n",
        "$$𝑑\n",
        "𝑦/\n",
        "𝑑\n",
        "𝑥\n",
        "=\n",
        "2\n",
        "dx/\n",
        "dy\n",
        "​\n",
        " =2$$\n",
        "\n",
        "So, the gradient will be a 2x2 matrix where each entry is 2.\n",
        "\n",
        "------------------------\n",
        "#Example\n",
        "Suppose x is randomly generated as:\n",
        "\n",
        "    x = [[0.5, 0.7],\n",
        "         [0.9, 0.2]]\n",
        "\n",
        "Then, y would be:\n",
        "\n",
        "    y = [[2 * 0.5 + 3, 2 * 0.7 + 3],\n",
        "         [2 * 0.9 + 3, 2 * 0.2 + 3]]\n",
        "       = [[4.0, 4.4],\n",
        "         [4.8, 3.4]]\n",
        "The gradient of y with respect to x is:\n",
        "\n",
        "    grad_of_y_wrt_x = [[2, 2],\n",
        "                       [2, 2]]\n",
        "\n",
        "------------------------------------                       \n",
        "# Output:\n",
        "\n",
        "    print(grad_of_y_wrt_x)  # Will print a 2x2 matrix of 2s\n",
        "This code will output:\n",
        "\n",
        "    <tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
        "     array([[2., 2.],\n",
        "           [2., 2.]], dtype=float32)>\n",
        "\n",
        "This happens because the derivative of\n",
        "\n",
        "$$y=2x+3$$\n",
        "\n",
        "with respect to each element of x is always 2."
      ],
      "metadata": {
        "id": "1Uc6BsXN32I6"
      }
    },
    {
      "source": [
        "import tensorflow as tf # Importing the TensorFlow library and assigning it the alias 'tf'\n",
        "\n",
        "W = tf.Variable(tf.random.uniform((2, 2)))\n",
        "b = tf.Variable(tf.zeros((2,)))\n",
        "x = tf.random.uniform((2, 2))\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "    y = tf.matmul(x, W) + b\n",
        "grad_of_y_wrt_W_and_b = tape.gradient(y, [W, b])\n"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "liOR8pv-rbVf"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compute the gradients of the output 𝑦\n",
        "\n",
        "y with respect to two variables: the weight matrix\n",
        "\n",
        "𝑊 and the bias vector 𝑏.\n",
        "\n",
        "Here's a step-by-step breakdown of what's happening:\n",
        "\n",
        "---------------------------\n",
        "# 1. W = tf.Variable(tf.random.uniform((2, 2)))\n",
        "You define a 2x2 matrix W, which serves as a weight matrix initialized with random values between 0 and 1 (from a uniform distribution).\n",
        "\n",
        "It's a trainable variable.\n",
        "\n",
        "The inner parentheses (2, 2) define the shape of the tensor.\n",
        "\n",
        "This is a tuple that specifies the dimensions of the tensor. In this case, you're creating a 2x2 matrix (2 rows and 2 columns).\n",
        "\n",
        "The outer parentheses are part of the function call to tf.random.uniform() or any other tensor creation function.\n",
        "\n",
        "The function itself requires arguments (like the shape), and the parentheses are used to pass those arguments.\n",
        "\n",
        "So the expression tf.random.uniform((2, 2)) uses:\n",
        "\n",
        "Inner parentheses (2, 2) to define the shape as a tuple.\n",
        "Outer parentheses as part of the function call to tf.random.uniform(), which generates the tensor.\n",
        "\n",
        "Without the inner parentheses, the function wouldn’t know what shape to create, and without the outer parentheses, you wouldn’t be able to call the function at all.\n",
        "\n",
        "In summary:\n",
        "\n",
        "The inner parentheses define the shape.\n",
        "The outer parentheses call the function to generate the tensor\n",
        "\n",
        "----------------------------------------\n",
        "#2. b = tf.Variable(tf.zeros((2,)))\n",
        "Here, you define a bias vector b of shape (2,).\n",
        "\n",
        "It's initialized with zeros and is also trainable.\n",
        "\n",
        "--------------------------------------------\n",
        "#3. x = tf.random.uniform((2, 2))\n",
        "You generate a 2x2 matrix x with random values between 0 and 1. This serves as the input data for the computation.\n",
        "\n",
        "-----------------\n",
        "#4. with tf.GradientTape() as tape:\n",
        "This block records the operations on W and b, allowing you to compute the gradient of the output with respect to these variables.\n",
        "\n",
        "------------------------\n",
        "#5. y = tf.matmul(x, W) + b\n",
        "This operation is a matrix multiplication between x and W, followed by the addition of the bias vector b.\n",
        "\n",
        "tf.matmul(x, W) performs the matrix multiplication.\n",
        "+ b adds the bias vector element-wise to each row of the result.\n",
        "\n",
        "So, for each row\n",
        "![image.png](data:image/png;base64,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)\n",
        "\n",
        "Where\n",
        "\n",
        "![image.png](data:image/png;base64,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)\n",
        "\n",
        "----------------------------------\n",
        "#6. grad_of_y_wrt_W_and_b = tape.gradient(y, [W, b])\n",
        "Here, you're computing the gradients of 𝑦 with respect to both W and b.\n",
        "\n",
        "This gives you the partial derivatives of 𝑦 with respect to each of the elements in W and b.\n",
        "\n",
        "##Gradient with respect to W:\n",
        "The gradient with respect to each element of W is computed based on how changes in W affect the output 𝑦.\n",
        "\n",
        "This is similar to backpropagation in neural networks.\n",
        "\n",
        "##Gradient with respect to b:\n",
        "Since the bias vector is added element-wise, the gradient with respect to each element of b will just be the sum of the gradients of the corresponding elements of the output.\n",
        "\n",
        "##Output:\n",
        "The output grad_of_y_wrt_W_and_b will be a list of two gradients:\n",
        "\n",
        "The gradient of 𝑦 with respect to W (a 2x2 matrix).\n",
        "\n",
        "The gradient of 𝑦 with respect to b (a 1D vector of length 2).\n",
        "Here’s an example of what the output might look like:\n",
        "\n",
        "    [<tf.Tensor: shape=(2, 2), dtype=float32, numpy=array([[...], [...]], dtype=float32)>,\n",
        "    <tf.Tensor: shape=(2,), dtype=float32, numpy=array([...], dtype=float32)>]\n",
        "\n",
        "The gradient values depend on the initial values of W, b, and x.\n",
        "\n",
        "These gradients indicate how each element of W and b would need to change to minimize a loss function in a typical machine learning scenario"
      ],
      "metadata": {
        "id": "9wR-5gr6rgr4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The GradientTape works with tensor operations"
      ],
      "metadata": {
        "id": "3e4B8JBB0Fho"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = tf.Variable(tf.random.uniform((2, 2)))\n",
        "with tf.GradientTape() as tape:\n",
        " y = 2 * x + 3\n",
        "grad_of_y_wrt_x = tape.gradient(y, x)"
      ],
      "metadata": {
        "id": "GFt9fVNR0C-m"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# It also works with lists of variables\n"
      ],
      "metadata": {
        "id": "B2dmExTG0aHM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "W = tf.Variable(tf.random.uniform((2, 2)))\n",
        "b = tf.Variable(tf.zeros((2,)))\n",
        "x = tf.random.uniform((2, 2))\n",
        "with tf.GradientTape() as tape:\n",
        " y = tf.matmul(x, W) + b\n",
        "grad_of_y_wrt_W_and_b = tape.gradient(y, [W, b])"
      ],
      "metadata": {
        "id": "wdG_6GlN0dp3"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using Tensorflow Classes"
      ],
      "metadata": {
        "id": "ZkunPSII5ZYx"
      }
    },
    {
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "class NaiveDense:\n",
        "    def __init__(self, input_size, output_size, activation):\n",
        "        self.activation = activation  # Assign the activation function\n",
        "        w_shape = (input_size, output_size)\n",
        "        w_initial_value = tf.random.uniform(w_shape, minval=0, maxval=1e-1)\n",
        ""
      ],
      "cell_type": "code",
      "metadata": {
        "id": "pO4VG0vJ5xPS"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. import tensorflow as tf\n",
        "\n",
        "This line imports the TensorFlow library and gives it the alias tf.\n",
        "\n",
        "TensorFlow is a powerful library for numerical computation, particularly used for building and training machine learning models.\n",
        "\n",
        "By using as tf, we can refer to TensorFlow functions and classes using tf. instead of writing out the full tensorflow each time.\n",
        "\n",
        "This is a common convention to keep code concise.\n",
        "\n",
        "-----------------------------------\n",
        "#2. class NaiveDense:\n",
        "\n",
        "This line defines a new class called NaiveDense.\n",
        "\n",
        "A class is like a blueprint for creating objects.\n",
        "\n",
        "This class will represent a simple, densely connected layer in a neural network.\n",
        "\n",
        "---------------------\n",
        "#3. def __init__(self, input_size, output_size, activation):\n",
        "\n",
        "This line defines the constructor (or initializer) of the NaiveDense class.\n",
        "\n",
        "##The __init__ method\n",
        "\n",
        "is automatically called when you create a new object (instance) of the class.\n",
        "\n",
        "It takes three arguments:\n",
        "\n",
        "###input_size:\n",
        "The number of input features to the layer.\n",
        "\n",
        "###output_size:\n",
        "The number of output neurons in the layer.\n",
        "\n",
        "###activation:\n",
        "The activation function to use for this layer (e.g., tf.nn.relu, tf.nn.sigmoid).\n",
        "\n",
        "-----------------------\n",
        "#4. self.activation = activation\n",
        "\n",
        "Inside the __init__ method, this line assigns the activation function passed as an argument to an attribute called self.activation.\n",
        "\n",
        "self refers to the current instance of the class.\n",
        "\n",
        "So, this line is essentially saying:\n",
        "##\"For this particular object of the NaiveDense class, store the given activation function in its activation attribute.\"\n",
        "\n",
        "This allows the activation function to be used later within the layer's operations.\n",
        "\n",
        "-----------------------------\n",
        "#5. w_shape = (input_size, output_size)\n",
        "\n",
        "This line creates a tuple called w_shape that represents the shape of the weight matrix for the layer.\n",
        "\n",
        "The weight matrix connects the inputs to the outputs of the layer.\n",
        "\n",
        "The shape is determined by input_size and output_size, ensuring the weight matrix has the correct dimensions for the connections.\n",
        "\n",
        "---------------------------\n",
        "#6. w_initial_value = tf.random.uniform(w_shape, minval=0, maxval=1e-1)\n",
        "\n",
        "This line uses TensorFlow's tf.random.uniform function to generate random values for initializing the weight matrix (W).\n",
        "\n",
        "w_shape is used to determine the shape of the matrix.\n",
        "\n",
        "minval=0 and maxval=1e-1 specify that the random values should be uniformly distributed between 0 and 0.1.\n",
        "\n",
        "This is a common practice for initializing weights in neural networks to help with training stability.\n",
        "\n",
        "##Note:\n",
        "1 × 10-1 = 1 / 10 = 0.1\n",
        "\n",
        "Therefore, 1e-1 translates to 0.1\n",
        "\n",
        "------------------------------------\n",
        "#In summary\n",
        "\n",
        "This section of the code sets up the basic structure of a simple dense layer in a neural network. It defines the layer's constructor, initializes the activation function, and generates initial random values for the weight matrix. These are fundamental steps in creating a functional neural network layer.\n"
      ],
      "metadata": {
        "id": "WRYWgC9b56eB"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP9E3VEOYy67w/vKPY0+NZq",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}